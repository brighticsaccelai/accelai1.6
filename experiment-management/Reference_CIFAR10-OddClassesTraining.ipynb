{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\" style=\"border-width:4px\">Resnet model training on CIFAR10 odd numbered classes on SBrain</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "THIS NOTEBOOK CAN TAKE LONG TIME TO RUN. SO IT IS MAINLY FOR REFERENCE. IT MAY TAKE SOME TIME IF YOU TRY TO RUN IT.\n",
    "\n",
    "In this notebook, we will build a Resnet model and train it on CIFAR10 dataset, but only on odd numbered classes. When we do transfer learning in <a href=\"2_TransferLearning.ipynb\">TransferLearning Notebook</a>, this model will serve as the model that we transfer from. \n",
    "\n",
    "Below, we define an input_function to be used in SBrain. More details on input_function is in <a href=\"2_TransferLearning.ipynb\">TransferLearning Notebook</a>.\n",
    "In this function, we download the CIFAR-10 data, load TFRecordDataset from it and filter it for only the odd numbered classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.learning.experiment import *\n",
    "from sbrain.dataset.dataset import *\n",
    "\n",
    "def only_odd_classes_input_function(mode, batch_size, params):\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import sys\n",
    "    import tarfile\n",
    "    import pickle\n",
    "    CLASS_INDEX_MOD = 0\n",
    "\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32\n",
    "    DEPTH = 3\n",
    "\n",
    "    CIFAR_FILENAME = 'cifar-10-python.tar.gz'\n",
    "    CIFAR_DOWNLOAD_URL = 'https://www.cs.toronto.edu/~kriz/' + CIFAR_FILENAME\n",
    "    CIFAR_LOCAL_FOLDER = 'cifar-10-batches-py'\n",
    "\n",
    "    def download_and_extract(data_dir):\n",
    "        # download CIFAR-10 if not already downloaded.\n",
    "        tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir,\n",
    "                                                      CIFAR_DOWNLOAD_URL)\n",
    "        tarfile.open(os.path.join(data_dir, CIFAR_FILENAME),\n",
    "                     'r:gz').extractall(data_dir)\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _get_file_names():\n",
    "        \"\"\"Returns the file names expected to exist in the input_dir.\"\"\"\n",
    "        file_names = {}\n",
    "        file_names['train'] = ['data_batch_%d' % i for i in range(1, 5)]\n",
    "        file_names['validation'] = ['data_batch_5']\n",
    "        file_names['eval'] = ['test_batch']\n",
    "        return file_names\n",
    "\n",
    "    def read_pickle_from_file(filename):\n",
    "        with tf.gfile.Open(filename, 'rb') as f:\n",
    "            if sys.version_info >= (3, 0):\n",
    "                data_dict = pickle.load(f, encoding='bytes')\n",
    "            else:\n",
    "                data_dict = pickle.load(f)\n",
    "        return data_dict\n",
    "\n",
    "    def convert_to_tfrecord(input_files, output_file):\n",
    "        \"\"\"Converts a file to TFRecords.\"\"\"\n",
    "        print('Generating %s' % output_file)\n",
    "        with tf.python_io.TFRecordWriter(output_file) as record_writer:\n",
    "            for input_file in input_files:\n",
    "                data_dict = read_pickle_from_file(input_file)\n",
    "                data = data_dict[b'data']\n",
    "                labels = data_dict[b'labels']\n",
    "                num_entries_in_batch = len(labels)\n",
    "                for i in range(num_entries_in_batch):\n",
    "                    example = tf.train.Example(features=tf.train.Features(\n",
    "                        feature={\n",
    "                            'image': _bytes_feature(data[i].tobytes()),\n",
    "                            'label': _int64_feature(labels[i])\n",
    "                        }))\n",
    "                    record_writer.write(example.SerializeToString())\n",
    "\n",
    "    def setup_cifar10_data(data_dir):\n",
    "        train_dir = os.path.join(data_dir, 'train.tfrecords')\n",
    "        validation_dir = os.path.join(data_dir, 'validation.tfrecords')\n",
    "        eval_dir = os.path.join(data_dir, 'eval.tfrecords')\n",
    "        if os.path.exists(train_dir) and os.path.exists(validation_dir) and os.path.exists(eval_dir):\n",
    "            print(\"Data already present.\")\n",
    "        else:\n",
    "            print('Download from {} and extract. Wait for download complete message..'.format(CIFAR_DOWNLOAD_URL))\n",
    "            download_and_extract(data_dir)\n",
    "            print('Download completed')\n",
    "            file_names = _get_file_names()\n",
    "            input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)\n",
    "            for mode, files in file_names.items():\n",
    "                input_files = [os.path.join(input_dir, f) for f in files]\n",
    "                output_file = os.path.join(data_dir, mode + '.tfrecords')\n",
    "                try:\n",
    "                    os.remove(output_file)\n",
    "                except OSError:\n",
    "                    pass\n",
    "                # Convert to tf.train.Example and write the to TFRecords.\n",
    "                convert_to_tfrecord(input_files, output_file)\n",
    "            print('Done!')\n",
    "\n",
    "    class Cifar10DataSet(object):\n",
    "        \"\"\"Cifar10 data set.\n",
    "        Described by http://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, data_dir, subset='train', use_distortion=True):\n",
    "            self.data_dir = data_dir\n",
    "            self.subset = subset\n",
    "            self.use_distortion = use_distortion\n",
    "\n",
    "        def get_filenames(self):\n",
    "            if self.subset in ['train', 'validation', 'eval']:\n",
    "                return [os.path.join(self.data_dir, self.subset + '.tfrecords')]\n",
    "            else:\n",
    "                raise ValueError('Invalid data subset \"%s\"' % self.subset)\n",
    "\n",
    "        def parser(self, serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "            # Dimensions of the images in the CIFAR-10 dataset.\n",
    "            # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n",
    "            # input format.\n",
    "            features = tf.parse_single_example(\n",
    "                serialized_example,\n",
    "                features={\n",
    "                    'image': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64),\n",
    "                })\n",
    "            image = tf.decode_raw(features['image'], tf.uint8)\n",
    "            image.set_shape([DEPTH * HEIGHT * WIDTH])\n",
    "\n",
    "            # Reshape from [depth * height * width] to [depth, height, width].\n",
    "            image = tf.cast(\n",
    "                tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]),\n",
    "                tf.float32)\n",
    "            label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "            # Custom preprocessing.\n",
    "            image = self.preprocess(image)\n",
    "\n",
    "            return ({\"data\": image}, label)\n",
    "\n",
    "        def adjust_labels(self, data, label):\n",
    "            return data, tf.floordiv(label, 2)\n",
    "\n",
    "        def filter_fun(self, data, label):\n",
    "            return tf.equal(tf.mod(label, 2), CLASS_INDEX_MOD)\n",
    "\n",
    "        def get_dataset(self):\n",
    "            \"\"\"Read the images and labels from 'filenames'.\"\"\"\n",
    "            filenames = self.get_filenames()\n",
    "\n",
    "            dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "            # Parse records.\n",
    "            dataset = dataset.map(self.parser).filter(self.filter_fun).map(self.adjust_labels)\n",
    "            return dataset\n",
    "\n",
    "        def preprocess(self, image):\n",
    "            \"\"\"Preprocess a single image in [height, width, depth] layout.\"\"\"\n",
    "            if self.subset == 'train' and self.use_distortion:\n",
    "                # Pad 4 pixels on each dimension of feature map, done in mini-batch\n",
    "                image = tf.image.resize_image_with_crop_or_pad(image, 40, 40)\n",
    "                image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH])\n",
    "                image = tf.image.random_flip_left_right(image)\n",
    "            return image\n",
    "\n",
    "    use_distortion = False\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        subset = 'train'\n",
    "        use_distortion = True\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        subset = 'validation'\n",
    "    else:\n",
    "        subset = 'eval'\n",
    "    data_dir = \"/workspace/shared-dir/sample-notebooks/demo-data/learning/OddClasses/\"\n",
    "\n",
    "    setup_cifar10_data(data_dir)\n",
    "\n",
    "    dataset = Cifar10DataSet(data_dir, subset, use_distortion).get_dataset()\n",
    "\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define, a model function where we build a RESNET model to be trained on the above dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_model_function(features, labels, mode, params):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    ######################################## Resnet model #########################################\n",
    "\n",
    "    class ResNet(object):\n",
    "        \"\"\"ResNet model.\"\"\"\n",
    "\n",
    "        def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):\n",
    "            \"\"\"ResNet constructor.\n",
    "\n",
    "            Args:\n",
    "              is_training: if build training or inference model.\n",
    "              data_format: the data_format used during computation.\n",
    "                           one of 'channels_first' or 'channels_last'.\n",
    "            \"\"\"\n",
    "            self._batch_norm_decay = batch_norm_decay\n",
    "            self._batch_norm_epsilon = batch_norm_epsilon\n",
    "            self._is_training = is_training\n",
    "            assert data_format in ('channels_first', 'channels_last')\n",
    "            self._data_format = data_format\n",
    "\n",
    "        def forward_pass(self, x):\n",
    "            raise NotImplementedError(\n",
    "                'forward_pass() is implemented in ResNet sub classes')\n",
    "\n",
    "        def _residual_v1(self,\n",
    "                         x,\n",
    "                         kernel_size,\n",
    "                         in_filter,\n",
    "                         out_filter,\n",
    "                         stride,\n",
    "                         activate_before_residual=False):\n",
    "            \"\"\"Residual unit with 2 sub layers, using Plan A for shortcut connection.\"\"\"\n",
    "\n",
    "            del activate_before_residual\n",
    "            with tf.name_scope('residual_v1') as name_scope:\n",
    "                orig_x = x\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, stride)\n",
    "                x = self._batch_norm(x)\n",
    "                x = self._relu(x)\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, 1)\n",
    "                x = self._batch_norm(x)\n",
    "\n",
    "                if in_filter != out_filter:\n",
    "                    orig_x = self._avg_pool(orig_x, stride, stride)\n",
    "                    pad = (out_filter - in_filter) // 2\n",
    "                    if self._data_format == 'channels_first':\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n",
    "                    else:\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n",
    "\n",
    "                x = self._relu(tf.add(x, orig_x))\n",
    "\n",
    "                tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "                return x\n",
    "\n",
    "        def _conv(self, x, kernel_size, filters, strides, is_atrous=False):\n",
    "            \"\"\"Convolution.\"\"\"\n",
    "\n",
    "            padding = 'SAME'\n",
    "            if not is_atrous and strides > 1:\n",
    "                pad = kernel_size - 1\n",
    "                pad_beg = pad // 2\n",
    "                pad_end = pad - pad_beg\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "                else:\n",
    "                    x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n",
    "                padding = 'VALID'\n",
    "            return tf.layers.conv2d(\n",
    "                inputs=x,\n",
    "                kernel_size=kernel_size,\n",
    "                filters=filters,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                use_bias=False,\n",
    "                data_format=self._data_format)\n",
    "\n",
    "        def _batch_norm(self, x):\n",
    "            if self._data_format == 'channels_first':\n",
    "                data_format = 'NCHW'\n",
    "            else:\n",
    "                data_format = 'NHWC'\n",
    "            return tf.contrib.layers.batch_norm(\n",
    "                x,\n",
    "                decay=self._batch_norm_decay,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                epsilon=self._batch_norm_epsilon,\n",
    "                is_training=self._is_training,\n",
    "                fused=True,\n",
    "                data_format=data_format)\n",
    "\n",
    "        def _relu(self, x):\n",
    "            return tf.nn.relu(x)\n",
    "\n",
    "        def _fully_connected(self, x, out_dim):\n",
    "            with tf.name_scope('fully_connected') as name_scope:\n",
    "                x = tf.layers.dense(x, out_dim)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _avg_pool(self, x, pool_size, stride):\n",
    "            with tf.name_scope('avg_pool') as name_scope:\n",
    "                x = tf.layers.average_pooling2d(\n",
    "                    x, pool_size, stride, 'SAME', data_format=self._data_format)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _global_avg_pool(self, x):\n",
    "            with tf.name_scope('global_avg_pool') as name_scope:\n",
    "                assert x.get_shape().ndims == 4\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.reduce_mean(x, [2, 3])\n",
    "                else:\n",
    "                    x = tf.reduce_mean(x, [1, 2])\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "    ####################################### end Resnet base model definition ######################################\n",
    "\n",
    "    ####################################### start cifar resnet subclassing ########################################\n",
    "\n",
    "    class ResNetCifar10(ResNet):\n",
    "        \"\"\"Cifar10 model with ResNetV1 and basic residual block.\"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                     num_layers,\n",
    "                     is_training,\n",
    "                     batch_norm_decay,\n",
    "                     batch_norm_epsilon,\n",
    "                     data_format='channels_first'):\n",
    "            super(ResNetCifar10, self).__init__(\n",
    "                is_training,\n",
    "                data_format,\n",
    "                batch_norm_decay,\n",
    "                batch_norm_epsilon\n",
    "            )\n",
    "            self.n = (num_layers - 2) // 6\n",
    "            # Add one in case label starts with 1. No impact if label starts with 0.\n",
    "            self.num_classes = 5 + 1\n",
    "            self.filters = [16, 16, 32, 64]\n",
    "            self.strides = [1, 2, 2]\n",
    "\n",
    "        def forward_pass(self, x, input_data_format='channels_last'):\n",
    "            \"\"\"Build the core model within the graph.\"\"\"\n",
    "            if self._data_format != input_data_format:\n",
    "                if input_data_format == 'channels_last':\n",
    "                    # Computation requires channels_first.\n",
    "                    x = tf.transpose(x, [0, 3, 1, 2])\n",
    "                else:\n",
    "                    # Computation requires channels_last.\n",
    "                    x = tf.transpose(x, [0, 2, 3, 1])\n",
    "\n",
    "            # Image standardization.\n",
    "            x = x / 128 - 1\n",
    "\n",
    "            x = self._conv(x, 3, 16, 1)\n",
    "            x = self._batch_norm(x)\n",
    "            x = self._relu(x)\n",
    "\n",
    "            # Use basic (non-bottleneck) block and ResNet V1 (post-activation).\n",
    "            res_func = self._residual_v1\n",
    "\n",
    "            # 3 stages of block stacking.\n",
    "            for i in range(3):\n",
    "                with tf.name_scope('stage'):\n",
    "                    for j in range(self.n):\n",
    "                        if j == 0:\n",
    "                            # First block in a stage, filters and strides may change.\n",
    "                            x = res_func(x, 3, self.filters[i], self.filters[i + 1],\n",
    "                                         self.strides[i])\n",
    "                        else:\n",
    "                            # Following blocks in a stage, constant filters and unit stride.\n",
    "                            x = res_func(x, 3, self.filters[i + 1], self.filters[i + 1], 1)\n",
    "\n",
    "            x = self._global_avg_pool(x)\n",
    "            x = self._fully_connected(x, self.num_classes)\n",
    "\n",
    "            return x\n",
    "    ####################################### end cifar resnet subclassing ########################################\n",
    "\n",
    "    ####################################### start loss etc definitions ##########################################\n",
    "\n",
    "    num_layers = 44\n",
    "    batch_norm_decay = 0.997\n",
    "    batch_norm_epsilon = 1e-5\n",
    "    weight_decay = 2e-4\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    model = ResNetCifar10(\n",
    "        num_layers,\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=\"channels_last\")\n",
    "\n",
    "    data = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(\"data\", shape=(32,32,3))])\n",
    "    data = tf.reshape(data, (-1,32,32,3))\n",
    "    logits = model.forward_pass(data, input_data_format='channels_last')\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': tf.argmax(input=logits, axis=1),\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    model_params = tf.trainable_variables()\n",
    "    loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "\n",
    "    ####################################### end loss etc definitions ############################################\n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, axis=1),\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    # TODO tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec( mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "    #FIXME\n",
    "    num_batches_per_epoch = 45000 // 64  # * num_workers)\n",
    "    boundaries = [ num_batches_per_epoch * x for x in np.array([82, 123, 300], dtype=np.int64)]\n",
    "    staged_lr = [learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "    learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(), boundaries, staged_lr)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    # optimizer = tf.train.SyncReplicasOptimizer(optimizer, 4, 4)\n",
    "    # optimizer.make_session_run_hook()\n",
    "\n",
    "    global_step = tf.train.get_global_step()\n",
    "    print(\"Device is\")\n",
    "    print(global_step.device)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.control_dependencies([train_op]):\n",
    "            train_op = tf.Print(global_step, [global_step])\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_chief_hooks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the job\n",
    "\n",
    "Below we run this setup for 5000 iterations and two workers. We save this model under the name \"Cifar10_Odd_Classes_5000_Iters\". Later in <a href=\"2_TransferLearning.ipynb\">TransferLearning Notebook</a>, we will look this up and transfer from this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator.NewClassificationEstimator(model_fn=cifar_model_function)\n",
    "name = \"CIFAR10_odd_class_estimator\"\n",
    "estimator = Estimator.create(name, \"Resnet Cifar10 estimator\", estimator)\n",
    "\n",
    "hyper_parameters = HParams(iterations=5000, batch_size=128)\n",
    "rc = RunConfig(no_of_ps=1, no_of_workers=2, run_eval=True, use_gpu=True)\n",
    "\n",
    "\n",
    "odd_class_experiment_name = \"Cifar10_Odd_Classes_5000_Iterations\"\n",
    "experiment = Experiment.run(experiment_name=odd_class_experiment_name,\n",
    "                     description=\"Cifar10 Odd classes Model\",\n",
    "                     estimator=estimator,\n",
    "                     hyper_parameters=hyper_parameters,\n",
    "                     run_config=rc,\n",
    "                     dataset_version_split=None,\n",
    "                     input_function=only_odd_classes_input_function)\n",
    "job = experiment.get_single_job()\n",
    "print(\"tensorboard url\")\n",
    "print(job.get_tensorboard_url())\n",
    "\n",
    "print(\"Has the job finished? {}\".format(job.has_finished()))\n",
    "\n",
    "job.wait_until_finish()\n",
    "\n",
    "print(\"Model metrics..\")\n",
    "print(job.get_model().model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}