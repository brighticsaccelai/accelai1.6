{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\" style=\"border-width:4px\">Best Model and Last Model </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we walk you through the new notion of \"best model\" vs \"last model\".\n",
    "\n",
    "SBrain will keep track of the best model if either of 'StopIfHigherHook' or 'StopIfLowerHook' is used in the model function. Below is an example, where we show case this for a simple MNIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.learning.experiment import *\n",
    "from sbrain.dataset.dataset import *\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "def input_function(mode, batch_size, params):\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    import tensorflow as tf\n",
    "\n",
    "    local_dir = \"/workspace/shared-dir/sample-notebooks/demo-data/learning/mnist/\"\n",
    "\n",
    "    if mode == \"train\":\n",
    "        mnist = input_data.read_data_sets(local_dir, one_hot=True)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(({\"data\" : mnist.train.images}, mnist.train.labels))\n",
    "        dataset = dataset.shuffle(1000).batch(batch_size).repeat()\n",
    "        return dataset\n",
    "    else:\n",
    "        mnist = input_data.read_data_sets(local_dir, one_hot=True)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(({\"data\" : mnist.test.images}, mnist.test.labels))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model function below, please note the use of 'StopIfHigherHook'. If no such hooks are given, SBrain will not keep track of the best model, since SBrain cannot understand what \"best\" means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_function_with_mnist(features, labels, mode, params):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    net = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(\"data\", shape=(784))])\n",
    "    # labels = tf.one_hot(labels, 2) ## either or\n",
    "    for units in [20, 20]:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # Compute logits (1 per class).\n",
    "    # logits = tf.layers.dense(net, 2, activation=None) ## either or\n",
    "    logits = tf.layers.dense(net, 10, activation=None)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "    labels_to_compare = tf.argmax(labels, 1)\n",
    "    accuracy = tf.metrics.accuracy(labels=labels_to_compare,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    global_step = tf.train.get_global_step()\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op,\n",
    "                                      training_chief_hooks=None,\n",
    "                                      training_hooks=[StopIfHigherHook(metric_name=\"accuracy\",\n",
    "                                                                       threshold=0.50, min_steps=20, run_every_secs=15)]\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit the job and wait for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator.NewClassificationEstimator(model_fn=my_model_function_with_mnist)\n",
    "name = \"BestModelEstimator\" + str(time.time()).replace(\".\", \"\")\n",
    "estimator = Estimator.create(name, \"Hello\", estimator)\n",
    "\n",
    "hyper_parameters = HParams(iterations=50000, batch_size=10)\n",
    "rc = RunConfig(no_of_ps=1, no_of_workers=1, summary_save_frequency=5000, run_eval=True, use_gpu=False, checkpoint_frequency_in_steps=500)\n",
    "\n",
    "exper = Experiment.run(experiment_name=\"BestModelEstimator\" + str(time.time()).replace(\".\", \"\"),\n",
    "                       description=\"Really first model\",\n",
    "                       estimator=estimator,\n",
    "                       hyper_parameters=hyper_parameters,\n",
    "                       run_config=rc,\n",
    "                       dataset_version_split=None,\n",
    "                       input_function=input_function)\n",
    "job = exper.get_single_job()\n",
    "print(job.__dict__)\n",
    "print(\"\")\n",
    "print(\"tensorboard url\")\n",
    "print(job.get_tensorboard_url())\n",
    "\n",
    "job.has_finished()\n",
    "\n",
    "job.wait_until_finish()\n",
    "\n",
    "print(\"Is the job success?? : {}\".format(job.is_success()))\n",
    "\n",
    "print()\n",
    "print(\"Model metrics..\")\n",
    "print(job.get_model().model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The produced model has an additional method to check if it has a \"best model weights\". model.has_best_model(). If the training happened without hooks, then this will return false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = job.get_model()\n",
    "print(\"Does this model have best model weights? - {}\".format(model.has_best_model()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there is additional boolean attribute 'best_model' added to deploy() method to identify which weights to deploy. Whether it is the best weights or the latest weights. \n",
    "\n",
    "If the model does not have best_model weights (if it has not been trained with the hooks), then it will throw an error if the attribute is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = model.deploy(\"MyEndpoint\" + str(time.time()).replace(\".\", \"\"),\n",
    "                  \"myDesc\", 1, best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = np.random.standard_normal((1, 784))\n",
    "ft = {\"features\":req}\n",
    "ac = me.predict(ft)\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model_checkpoint = model.get_model_checkpoint(best_model=False)\n",
    "best_model_checkpoint = model.get_model_checkpoint(best_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheers!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}