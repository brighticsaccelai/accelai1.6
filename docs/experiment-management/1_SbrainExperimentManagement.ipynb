{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\" style=\"border-width:4px\">SBrain Learning Experiment Management Tutorial </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will explore some of the **SBrain** features related to model learning. **SBrain** takes the deep learning model code written in your notebook and runs it in a cluster as a distributed job, accelerating the learning and allowing data scientists to run more experiments is the same period of time. \n",
    " **SBrain** provides the data scientist with high level abstractions hiding the engineering details such as cluster allocation, tensorflow distributed synchronization and job management.\n",
    "\n",
    "\n",
    " In this notebook, we will train a Resnet model from scratch using CIFAR-10 data. Here are the tutorial goals:\n",
    "\n",
    " 1. Get you familiarized with the main **SBrain** features and abstractions related to learning\n",
    " 2. Walk you through defining estimators, which encapsulate your model architecture and can be shared and reused by others\n",
    " 3. Showcase how you can manage/monitor distributed learning jobs running in a cluster, from the notebook\n",
    " 4. Showcase the benefit of submitting parallel jobs in a cluster configured in a declarative manner, as opposed to running the learning in your own computer;\n",
    " 5. Configure hyperparameter search space and run multiple jobs in parallel with different configurations and evaluate the metrics for each\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try it out\n",
    "\n",
    "Before we begin, it would be good to copy this notebook and rename it with your name at the end, since we don't want multiple people editing the same notebook at the same time, causing reloading issues.\n",
    "\n",
    "#### Imports\n",
    "After that, lets start by importing the necessary packages, mainly learning and dataset. The learning package contains everything related to defining a model learning experiment.\n",
    "The dataset package contains everything related to defining dataset transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.learning.experiment import *\n",
    "from sbrain.dataset.dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Names\n",
    "\n",
    "Most of the **SBrain** artifacts you would create in this notebook like estimators, models and jobs, need to have a human readable unique name which others can look up and possibly reuse or inspect. We provide you a helper function here to make the names unique by appending the username and a timestamp at the end, so that you don't run into DuplicateName error every now and then. You can turn this off by changing the flag should_uniquify to False. Please, put your username as the value for the user_name field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"albin\"\n",
    "\n",
    "def uniquify(name):\n",
    "    import time\n",
    "    should_uniquify = True\n",
    "    if should_uniquify:\n",
    "        return name + user_name + str(time.time()).replace(\".\",\"\")\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SBrain Estimators\n",
    "\n",
    "\n",
    "**SBrain** exposes **Estimators** as the main abstraction for model definition, evaluation and depoyment. They are higher level APIs which hide a lot of low level details. **SBrain** follows the same style as\n",
    "TensorFlow Estimators.\n",
    "More details on TensorFlow Estimator APIs are given in the following links.\n",
    "- https://www.tensorflow.org/programmers_guide/estimators\n",
    "- https://www.tensorflow.org/get_started/custom_estimators\n",
    "\n",
    "The advantage is that anybody who uses TensorFlow can directly plug in their code into **SBrain** and immediately leverage all the capabilities that **SBrain** provides.\n",
    "\n",
    "#### The model function\n",
    "\n",
    "One important function that the user has to implement when using estimators is the model_function. In the model_function, we can define the architecture of the neural network, the loss, the training operation etc, and we return a tf.estimator.EstimatorSpec() object with everything that we have defined.\n",
    "\n",
    "#### Our model function : ResNet and CIFAR-10\n",
    "\n",
    " In the below code, we are defining a ResNet architecture and adapting it to CIFAR-10 dataset. Towards the very end of this big function, we are definining the network, loss, optimizer and training operation. This neural network has 44 layers and is a fairly complex neural network to train.\n",
    "\n",
    " Since, **SBrain** captures this function and executes it in a cluster, references to any libraries that you may use inside this function should be imported on the top inside the function. This helps you isolate your notebook environment/variables from your cluster code context.\n",
    "\n",
    " More comments are provided all throughout the code below. For more details on TensorFlow model_function abstraction please read the below article.\n",
    "\n",
    " - https://www.tensorflow.org/get_started/custom_estimators#write_a_model_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_model_function(features, labels, mode, params):\n",
    "    ## Importing relevant packages\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    ########## Defining ResNet structure as a class. #############\n",
    "\n",
    "    class ResNet(object):\n",
    "        \"\"\"ResNet model.\"\"\"\n",
    "\n",
    "        def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):\n",
    "            \"\"\"ResNet constructor.\n",
    "\n",
    "            Args:\n",
    "              is_training: if build training or inference model.\n",
    "              data_format: the data_format used during computation.\n",
    "                           one of 'channels_first' or 'channels_last'.\n",
    "            \"\"\"\n",
    "            self._batch_norm_decay = batch_norm_decay\n",
    "            self._batch_norm_epsilon = batch_norm_epsilon\n",
    "            self._is_training = is_training\n",
    "            assert data_format in ('channels_first', 'channels_last')\n",
    "            self._data_format = data_format\n",
    "\n",
    "        def forward_pass(self, x):\n",
    "            raise NotImplementedError(\n",
    "                'forward_pass() is implemented in ResNet sub classes')\n",
    "\n",
    "        def _residual_v1(self,\n",
    "                         x,\n",
    "                         kernel_size,\n",
    "                         in_filter,\n",
    "                         out_filter,\n",
    "                         stride,\n",
    "                         activate_before_residual=False):\n",
    "            \"\"\"Residual unit with 2 sub layers, using Plan A for shortcut connection.\"\"\"\n",
    "\n",
    "            del activate_before_residual\n",
    "            with tf.name_scope('residual_v1') as name_scope:\n",
    "                orig_x = x\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, stride)\n",
    "                x = self._batch_norm(x)\n",
    "                x = self._relu(x)\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, 1)\n",
    "                x = self._batch_norm(x)\n",
    "\n",
    "                if in_filter != out_filter:\n",
    "                    orig_x = self._avg_pool(orig_x, stride, stride)\n",
    "                    pad = (out_filter - in_filter) // 2\n",
    "                    if self._data_format == 'channels_first':\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n",
    "                    else:\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n",
    "\n",
    "                x = self._relu(tf.add(x, orig_x))\n",
    "\n",
    "                tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "                return x\n",
    "\n",
    "        def _conv(self, x, kernel_size, filters, strides, is_atrous=False):\n",
    "            \"\"\"Convolution.\"\"\"\n",
    "\n",
    "            padding = 'SAME'\n",
    "            if not is_atrous and strides > 1:\n",
    "                pad = kernel_size - 1\n",
    "                pad_beg = pad // 2\n",
    "                pad_end = pad - pad_beg\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "                else:\n",
    "                    x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n",
    "                padding = 'VALID'\n",
    "            return tf.layers.conv2d(\n",
    "                inputs=x,\n",
    "                kernel_size=kernel_size,\n",
    "                filters=filters,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                use_bias=False,\n",
    "                data_format=self._data_format)\n",
    "\n",
    "        def _batch_norm(self, x):\n",
    "            if self._data_format == 'channels_first':\n",
    "                data_format = 'NCHW'\n",
    "            else:\n",
    "                data_format = 'NHWC'\n",
    "            return tf.contrib.layers.batch_norm(\n",
    "                x,\n",
    "                decay=self._batch_norm_decay,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                epsilon=self._batch_norm_epsilon,\n",
    "                is_training=self._is_training,\n",
    "                fused=True,\n",
    "                data_format=data_format)\n",
    "\n",
    "        def _relu(self, x):\n",
    "            return tf.nn.relu(x)\n",
    "\n",
    "        def _fully_connected(self, x, out_dim):\n",
    "            with tf.name_scope('fully_connected') as name_scope:\n",
    "                x = tf.layers.dense(x, out_dim)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _avg_pool(self, x, pool_size, stride):\n",
    "            with tf.name_scope('avg_pool') as name_scope:\n",
    "                x = tf.layers.average_pooling2d(\n",
    "                    x, pool_size, stride, 'SAME', data_format=self._data_format)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _global_avg_pool(self, x):\n",
    "            with tf.name_scope('global_avg_pool') as name_scope:\n",
    "                assert x.get_shape().ndims == 4\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.reduce_mean(x, [2, 3])\n",
    "                else:\n",
    "                    x = tf.reduce_mean(x, [1, 2])\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "    ########## End ResNet class #############\n",
    "\n",
    "    ####### Subclassing ResNet specific to CIFAR-10 ###########\n",
    "\n",
    "    class ResNetCifar10(ResNet):\n",
    "        \"\"\"Cifar10 model with ResNetV1 and basic residual block.\"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                     num_layers,\n",
    "                     is_training,\n",
    "                     batch_norm_decay,\n",
    "                     batch_norm_epsilon,\n",
    "                     data_format='channels_first'):\n",
    "            super(ResNetCifar10, self).__init__(\n",
    "                is_training,\n",
    "                data_format,\n",
    "                batch_norm_decay,\n",
    "                batch_norm_epsilon\n",
    "            )\n",
    "            self.n = (num_layers - 2) // 6\n",
    "            # Add one in case label starts with 1. No impact if label starts with 0.\n",
    "            self.num_classes = 10\n",
    "            self.filters = [16, 16, 32, 64]\n",
    "            self.strides = [1, 2, 2]\n",
    "\n",
    "        def forward_pass(self, x, input_data_format='channels_last'):\n",
    "            \"\"\"Build the core model within the graph.\"\"\"\n",
    "            if self._data_format != input_data_format:\n",
    "                if input_data_format == 'channels_last':\n",
    "                    # Computation requires channels_first.\n",
    "                    x = tf.transpose(x, [0, 3, 1, 2])\n",
    "                else:\n",
    "                    # Computation requires channels_last.\n",
    "                    x = tf.transpose(x, [0, 2, 3, 1])\n",
    "\n",
    "            # Image standardization.\n",
    "            x = x / 128 - 1\n",
    "\n",
    "            x = self._conv(x, 3, 16, 1)\n",
    "            x = self._batch_norm(x)\n",
    "            x = self._relu(x)\n",
    "\n",
    "            # Use basic (non-bottleneck) block and ResNet V1 (post-activation).\n",
    "            res_func = self._residual_v1\n",
    "\n",
    "            # 3 stages of block stacking.\n",
    "            for i in range(3):\n",
    "                with tf.name_scope('stage'):\n",
    "                    for j in range(self.n):\n",
    "                        if j == 0:\n",
    "                            # First block in a stage, filters and strides may change.\n",
    "                            x = res_func(x, 3, self.filters[i], self.filters[i + 1],\n",
    "                                         self.strides[i])\n",
    "                        else:\n",
    "                            # Following blocks in a stage, constant filters and unit stride.\n",
    "                            x = res_func(x, 3, self.filters[i + 1], self.filters[i + 1], 1)\n",
    "\n",
    "            x = self._global_avg_pool(x)\n",
    "            x = self._fully_connected(x, self.num_classes)\n",
    "\n",
    "            return x\n",
    "    ####### End ResNetCifar10 class ###########\n",
    "\n",
    "    ######### Here we define all the hyperparameters, network, loss, optimzier and training operations ##################\n",
    "\n",
    "    ## Hyperparams\n",
    "    num_layers = 44\n",
    "\n",
    "    # batch_norm_decay = 0.997\n",
    "    batch_norm_decay = params[\"batch_norm_decay\"]\n",
    "    batch_norm_epsilon = 1e-5\n",
    "    # weight_decay = 2e-4\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    ## Neural network architecture\n",
    "    model = ResNetCifar10(\n",
    "        num_layers,\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=\"channels_last\")\n",
    "\n",
    "    data = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(\"data\", shape=(32,32,3))])\n",
    "    data = tf.reshape(data, (-1,32,32,3))\n",
    "    logits = model.forward_pass(data, input_data_format='channels_last')\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': tf.argmax(input=logits, axis=1),\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    ## Defining Loss\n",
    "    labels = tf.string_to_number(labels,out_type=tf.int32)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    model_params = tf.trainable_variables()\n",
    "    loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "\n",
    "    ## Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, axis=1),\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec( mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    ## Create optimizer\n",
    "    num_batches_per_epoch = 45000 // 64\n",
    "    boundaries = [ num_batches_per_epoch * x for x in np.array([82, 123, 300], dtype=np.int64)]\n",
    "    staged_lr = [learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "    learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(), boundaries, staged_lr)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    ## Create global step and training operation\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    ## Some print operations for better logging.\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.control_dependencies([train_op]):\n",
    "            train_op = tf.Print(predictions, [predictions, tf.shape(predictions), \"predictions\"], summarize=32)\n",
    "            train_op = tf.Print(global_step, [global_step])\n",
    "\n",
    "    ## Return Estimator Spec with loss and training operation\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_chief_hooks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More SBrain Abstractions\n",
    "\n",
    "The model function we defined above captures the structure of the network, loss and training operation. The next step is to tie this up to other **SBrain** abstractions.\n",
    "\n",
    "Here, we define a new **SBrain** classification estimator, passing in the same model_function that we defined earlier. This gives us an **SBrain** object which packages your model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_estimator = Estimator.NewClassificationEstimator(model_fn=cifar_model_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save this estimator as an asset in the **SBrain** environment, with a name we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = uniquify(\"MyFirstResnetCifar10Estimator\")\n",
    "saved_estimator = Estimator.create(estimator_name=name,\n",
    "                                   description=\"ResNet Cifar10 estimator trial\",\n",
    "                                   estimator_obj=classification_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have created an estimator in the **SBrain** environment. Anybody can look it up and reuse the same estimator with different hyper parameters and run configurations to try out different variants of the same experiment. We will walk you through it below later.\n",
    "\n",
    "#### Listing estimators\n",
    "If you would like to see the estimators that you or others created, use the below code to list them. This will list all the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_estimators = Estimator.list_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching Estimators\n",
    "Also you can search for specific ones using name/description as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resnet_estimators = Estimator.search(description=\"ResNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameters\n",
    "\n",
    "Now, let us define the hyper parameters. \n",
    "The first two parameters, 'iterations' and 'batch_size' are used by **SBrain** to initiate the training. But the rest of them are passed in to the model function that you defined earlier\n",
    " as params, so that you can dynamically adapt to the hyper parameters provided at runtime.\n",
    " \n",
    " Hyperparametes can be specified in 2 ways:\n",
    " \n",
    " 1. specify the constant values for all the parameters using the abstraction HParams as shown below:\n",
    " \n",
    " ```python\n",
    " hyper_parameters = HParams(iterations=5000,\n",
    "                           batch_size=32,\n",
    "                           batch_norm_decay=0.9,\n",
    "                           batch_norm_epsilon=1e-5,\n",
    "                           weight_decay=2e-4,\n",
    "                           learning_rate=0.1)\n",
    "```\n",
    "                           \n",
    " 2. specify an array of values for the parameters to try, using the HyperParamsSpace abstraction.\n",
    " The experiment will generate all combinations for the parameters and run a job for each combination.\n",
    " \n",
    " ```python\n",
    " space = HyperParamsSpace([\n",
    "        HParamValues.discrete_list(HParamValues.ITERATIONS, [500, 1000]),\n",
    "        HParamValues.constant_value(HParamValues.BATCH_SIZE, 128),\n",
    "        HParamValues.constant_value(\"weight_decay\", 2e-4),\n",
    "        HParamValues.discrete_list(\"batch_norm_decay\", [0.997, 0.99])\n",
    "    ])\n",
    "    grid = space.grid_search()\n",
    "```\n",
    "\n",
    "In this tutorial we are going to try the hyperparameter search using the HyperParamsSpace abstraction.  We specify only a few values to reduce the number of combinations. Also we run for very small number of iterations 500 and 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = HyperParamsSpace([\n",
    "        HParamValues.discrete_list(HParamValues.ITERATIONS, [500, 1000]),\n",
    "        HParamValues.constant_value(HParamValues.BATCH_SIZE, 128),\n",
    "        HParamValues.constant_value(\"weight_decay\", 2e-4),\n",
    "        HParamValues.discrete_list(\"batch_norm_decay\", [0.997, 0.99])\n",
    "    ])\n",
    "\n",
    "grid = space.grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Config\n",
    "\n",
    "Run Config lets you define how this job should be run on the cluster. You can specify the following parameters\n",
    "- **no_of_ps** : no of parameters servers to run. In the current release this should be at least 1.\n",
    "- **no_of_workers** : no of workers decide the parallelism we want during training. Try 1 for now. Later we will try 2. For the current cluster this is limited to 2, but this can be bigger.\n",
    "- **summary_save_frequency** - How often you should see the summary of your metrics in the tensorboard.\n",
    "- **run_eval** - If True, will run evaluation in parallel to training, and report an evaluation graph along with the training one in the tensorboard\n",
    "- **use_gpu** - If True, will run your job on a GPU machine in the cluster.\n",
    "\n",
    "All the relevant execution configurations can be declaratively defined in this single line. You can control your training parallelism, CPU/GPU, eval and summary saving all\n",
    "in this single declarative definition.\n",
    "\n",
    "All learning jobs under this experiment will use the same run configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfig(no_of_ps=1, no_of_workers=1, summary_save_frequency=10, run_eval=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSetSplit\n",
    "\n",
    "Now we should tie this to the data. Dataset is an **SBrain** abstraction for operations on large sets of data. For training purpose, we pre-created a DataSetSplit which is division of a DataSet into train, validation and eval sets. Refer to the DataSet notebook [DataSetManagement-Basic.ipynb](../dataset-management/DataSetManagement-Basic.ipynb) for details on how they are created. \n",
    "\n",
    " Now we look up a split that we have already created for CIFAR-10 data. The split ratio is **68,16,16** to be consistent with the CIFAR-10 **train,validation,eval** split.\n",
    " \n",
    " **NOTE**: If running the cell below returns error \"Split not found\", please run the [Cifar10 DataPreparation For Learning Tutorials.ipynb](./Cifar10%20DataPreparation%20For%20Learning%20Tutorials.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_10_split = DataSetSplit.lookup(dataset_name=\"cifar10-demo\", dataset_version_name=\"v1\", split_name=\"cifar-10-split\")\n",
    "print(cifar_10_split.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Experiment\n",
    "\n",
    "Now it is time to run this experiment. Here we tie everything that we have created and define an experiment.\n",
    "Give it a model name that makes sense, so that you can refer to it later. The attributes are given below.\n",
    " - **model_name** : Same name for the model and LearningJob that is created that we can refer to later to be deployed or inspected.\n",
    " - **description** : Description for the model/job\n",
    " - **estimator** : A saved estimator we already created.\n",
    " - **hparams_search_settings** : Hyper parameter search settings\n",
    " - **run_config** : The run configuration\n",
    " - **dataset_version_split** : split for feeding in data.\n",
    "\n",
    "This code returns a LearningJob object. Go ahead and execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = uniquify(\"Resnet_CIFAR10_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment.run(experiment_name=experiment_name,\n",
    "                                description=\"ResNet Model trained on Cifar10 data\",\n",
    "                                estimator=saved_estimator,\n",
    "                                hparams_search_settings=grid,\n",
    "                                run_config=run_config,\n",
    "                                dataset_version_split=cifar_10_split,\n",
    "                                input_function=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>At this point, you have started an experiment run. It will start multiple training jobs in **SBrain**. It is currently executing in the cluster. The Experiment object is your handle to the training job/s that are currently running in the cluster which are part of the particular experiment. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.has_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below methods will report the statuses of number of jobs under this experiment. List jobs will list all jobs under this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.report_status()\n",
    "\n",
    "experiment.list_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment will currently spawn jobs in parallel (currently two at a time). Once one job finishes, if there are more to run it will start the next one.\n",
    "\n",
    "<font color=red>**IMPORTANT**</font> : Currently, the cluster resources are limited and if multiple people are running at the same time, the cluster will wait for earlier submitted experiments to finish\n",
    "for yours to get started. This may mean the following:\n",
    "- The tensorboard link may take too long respond, becuase tensorboard is waiting for resource to execute.\n",
    "- Even if the tensorboard has come up, other details like graph/scalars may take too long to come up because training is waiting for resources.\n",
    "\n",
    "Eventually, the experiment will be scheduled and the training should proceed.\n",
    "\n",
    "<font color=blue>**Coming Soon**</font> : In future, a web UI will be provided for experiment and jobs monitoring and inspection of various logs (for eg. print statements in model function).\n",
    "Also, further experiment management features like cancelling will be exposed.\n",
    "\n",
    "#### LearningJob  Handle and Tensorboard Link\n",
    "\n",
    "Lets look up a specific job under this experiment for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = LearningJob.lookup(name=\"{}_Trial_Job-1\".format(experiment_name))\n",
    "print(\"tensorboard url\")\n",
    "print(job.get_tensorboard_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try the above tensorboard link in a new tab. This tensorboard gives you access to the metrics/graph of the currently running training.\n",
    "\n",
    "In the tensorboard, look for the 'Scalars' tab for all the metrics to appear there. The 'Graph' tab shows the ResNet graph that you have built. It may take a while (may be 5 minutes) for the 'Scalars' tab to appear based on the speed of metrics being written. Just keep refreshing the tensorboard until it appears.\n",
    "\n",
    "For any issues you might encounter, read below and watch out for sections marked <font color=red>**IMPORTANT**</font> for troubleshooting.\n",
    "\n",
    "Keep watching/refreshing the scalars tab to see how the loss and accuracy are proceeding in both training and validation. This way we can compare accuracy/loss between training and\n",
    "validation so that we can look for overfitting. Any other metrics that you define in the model function will show up on the tensorboard.\n",
    "\n",
    "Let's query the job for its status and see if it is finished.\n",
    "\n",
    "\n",
    "Below, we wait for the all the jobs under this experiment to finish. Anytime, there is a change in progress, this method notifies you. It is a blocking call and waits for all the jobs under this experiment to finish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.wait_until_finish()\n",
    "experiment.list_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completion, you can verify whether it failed or succeeded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.report_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}