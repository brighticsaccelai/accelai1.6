{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\" style=\"border-width:4px\">Batch Inference and Inference Jobs API </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we walk you through the batch inference api for a model endpoint. Towards the end we also create an inference job.\n",
    "\n",
    "Below we build a simple MNIST model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.learning.experiment import *\n",
    "from sbrain.dataset.dataset import *\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "def input_function(mode, batch_size, params):\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    import tensorflow as tf\n",
    "\n",
    "    local_dir = \"/workspace/shared-dir/sample-notebooks/demo-data/learning/mnist/\"\n",
    "\n",
    "    if mode == \"train\":\n",
    "        mnist = input_data.read_data_sets(local_dir, one_hot=True)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(({\"data\" : mnist.train.images}, mnist.train.labels))\n",
    "        dataset = dataset.shuffle(1000).batch(batch_size).repeat()\n",
    "        return dataset\n",
    "    else:\n",
    "        mnist = input_data.read_data_sets(local_dir, one_hot=True)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(({\"data\" : mnist.test.images}, mnist.test.labels))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_function_with_mnist(features, labels, mode, params):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    net = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(\"data\", shape=(784))])\n",
    "    # labels = tf.one_hot(labels, 2) ## either or\n",
    "    for units in [20, 20]:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # Compute logits (1 per class).\n",
    "    # logits = tf.layers.dense(net, 2, activation=None) ## either or\n",
    "    logits = tf.layers.dense(net, 10, activation=None)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "    labels_to_compare = tf.argmax(labels, 1)\n",
    "    accuracy = tf.metrics.accuracy(labels=labels_to_compare,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    global_step = tf.train.get_global_step()\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op,\n",
    "                                      training_chief_hooks=None,\n",
    "                                      training_hooks=[StopIfHigherHook(metric_name=\"accuracy\",\n",
    "                                                                       threshold=0.50, min_steps=20, run_every_secs=15)]\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit the job and wait for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator.NewClassificationEstimator(model_fn=my_model_function_with_mnist)\n",
    "name = \"BestModelEstimator\" + str(time.time()).replace(\".\", \"\")\n",
    "estimator = Estimator.create(name, \"Hello\", estimator)\n",
    "\n",
    "hyper_parameters = HParams(iterations=5000, batch_size=10)\n",
    "rc = RunConfig(no_of_ps=1, no_of_workers=1, summary_save_frequency=5000, run_eval=True, use_gpu=False, checkpoint_frequency_in_steps=500)\n",
    "\n",
    "exper = Experiment.run(experiment_name=\"BestModelEstimator\" + str(time.time()).replace(\".\", \"\"),\n",
    "                       description=\"Really first model\",\n",
    "                       estimator=estimator,\n",
    "                       hyper_parameters=hyper_parameters,\n",
    "                       run_config=rc,\n",
    "                       dataset_version_split=None,\n",
    "                       input_function=input_function)\n",
    "job = exper.get_single_job()\n",
    "print(job.__dict__)\n",
    "print(\"\")\n",
    "print(\"tensorboard url\")\n",
    "print(job.get_tensorboard_url())\n",
    "\n",
    "job.has_finished()\n",
    "\n",
    "job.wait_until_finish()\n",
    "\n",
    "print(\"Is the job success?? : {}\".format(job.is_success()))\n",
    "\n",
    "print()\n",
    "print(\"Model metrics..\")\n",
    "print(job.get_model().model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = job.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deploy a model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = model.deploy(\"MyEndpoint\" + str(time.time()).replace(\".\", \"\"),\n",
    "                  \"myDesc\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to run batch inference, we define two functions. \n",
    "\n",
    "1. Predict input function : This function has the same semantics as the input function to tensorflow estimator. This is not an SBrain input function. This function will be handed off to the tensorflow estimator directly, without any modification from the SBrain side. You have full control on how the data is fed into the model.\n",
    "2. Predict output function : The result of the above prediction will be passed into the output function for further processing. Further processing could mean, a: Writing them to shared directory for the application to process. b: Serialializing them in some format, so that it gets output to the response stream of the rest call. This function is expected to return a string, which will get passed in to the response of the rest call.\n",
    "\n",
    "NOTE: Let us know, if you need any additional libraries to be present to do the serialization that you might need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input_function():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    import tensorflow as tf\n",
    "    local_dir = \"/workspace/shared-dir/sample-notebooks/demo-data/learning/mnist/\"\n",
    "    print(\"In my predict input function\")\n",
    "    mnist = input_data.read_data_sets(local_dir, one_hot=True)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"data\" : mnist.train.images[0:10, :]}))\n",
    "    # dataset = tf.data.Dataset.from_tensor_slices(({\"data\" : mnist.train.images}, mnist.train.labels))\n",
    "    return dataset.batch(1000)\n",
    "\n",
    "\n",
    "\n",
    "def predict_output_function(results):\n",
    "    import pickle\n",
    "    import base64\n",
    "    print(\"BEFORE\")\n",
    "    ret = [] ## collecting all results of the model inference in an array\n",
    "    bc = 1\n",
    "    for each in results:\n",
    "        ret.append(each)\n",
    "        bc += 1\n",
    "    print(\"AFTER\")\n",
    "    serialized = pickle.dumps(ret, protocol=0) # protocol 0 is printable ASCII\n",
    "    s = base64.b64encode(serialized)\n",
    "    s = s.decode(\"UTF-8\")\n",
    "    ## WRITE RESULT TO /workspace/shared-dir/<some sub folder> to be used from outside.\n",
    "    \n",
    "    return s ## returning the serialized string to be returned to the application. You can also pass empty string response,\n",
    "    ## if you write directly to file system instead of the REST response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new api, which takes the new functions, which will be directly applied to the model without any processing from SBrain.\n",
    "\n",
    "The response string that the predict_output_function returns will be returned from the raw_predict() also. You can choose any kind of serialization in the output function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = me.raw_predict(predict_input_function, predict_output_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the serialized string, in this case, we use base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deserializing the base64 encoded string and getting back the original numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import base64\n",
    "result_numpy_array = pickle.loads(base64.decodestring(bytearray(res, encoding=\"UTF-8\")))\n",
    "print(result_numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference jobs\n",
    "\n",
    "Below is the api to submit an inference job. An inference job also has the same semantics of raw_predict(), which means the input function is directly handed off to the estimator without any modification from SBrain. In the case of the job, the inference instance comes up as a separate pod, that invokes the prediction and goes down. It comes up on a gpu node if gpu_required flag is set.\n",
    "\n",
    "You can write the output of prediction to any shared-dir folder to be used later, from the predict_output_function. A rest interface will not work, since this is a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input_function_job(params):\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    import tensorflow as tf\n",
    "    print(params)\n",
    "    local_dir = \"/workspace/shared-dir/sample-notebooks/demo-data/learning/mnist/\"\n",
    "    print(\"In my predict input function\")\n",
    "    mnist = input_data.read_data_sets(local_dir, one_hot=True)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"data\" : mnist.train.images[0:10, :]}))\n",
    "    # dataset = tf.data.Dataset.from_tensor_slices(({\"data\" : mnist.train.images}, mnist.train.labels))\n",
    "    return dataset.batch(1000)\n",
    "\n",
    "\n",
    "\n",
    "def predict_output_function_job(results, params):\n",
    "    import pickle\n",
    "    import base64\n",
    "    print(\"BEFORE\")\n",
    "    print(params)\n",
    "    ret = [] ## collecting all results of the model inference in an array\n",
    "    bc = 1\n",
    "    for each in results:\n",
    "        log.info(\"Processed batch {}\".format(bc))\n",
    "        ret.append(each)\n",
    "        bc += 1\n",
    "    print(\"AFTER\")\n",
    "    serialized = pickle.dumps(ret, protocol=0) # protocol 0 is printable ASCII\n",
    "    s = base64.b64encode(serialized)\n",
    "    s = s.decode(\"UTF-8\")\n",
    "    ## WRITE RESULT TO /workspace/shared-dir/<some sub folder> to be used from outside.\n",
    "    \n",
    "    return s ## returning the serialized string to be returned to the application. You can also pass empty string response,\n",
    "    ## if you write directly to file system instead of the REST response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_job = model.submit_inference_job(job_name=\"TestInfJob\" + str(time.time()).replace(\".\", \"\"),\n",
    "                           description=\"Test Job\",\n",
    "                           input_function=predict_input_function_job,\n",
    "                           output_function=predict_output_function_job,\n",
    "                           model_function=my_model_function_with_mnist, ## Try with and without the model function\n",
    "                           best_model=False\n",
    "                           ,gpu_required=True,\n",
    "                           params={\"testparam\":\"test_val_1\"}\n",
    "                               )\n",
    "\n",
    "\n",
    "inf_job.wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ModelInferenceJob.retrieve(inf_job.model_inference_job_id).is_success())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create one more job using the above cells.\n",
    "\n",
    "inf_job.request_cancellation() ## Immediately returns, later the job will be cancelled.\n",
    "\n",
    "\n",
    "inf_job.cancel() ## requests cancel and waits for cancellation to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheers!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}