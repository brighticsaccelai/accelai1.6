{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\" style=\"border-width:4px\">SBrain Transfer Learning Tutorial </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial will walk you through, how transfer learning can be done using **SBrain**. As a primer, please go through the Experiment Management learning notebook before going through this <a href=\"1_SbrainExperimentManagement.ipynb\">notebook</a>\n",
    "\n",
    "In this notebook, we go through a simple Transfer Learning usecase, where we transfer the weights from one model built in the past, on **SBrain**, to another model that we build in this notebook.\n",
    "\n",
    "The first model was built with a RESNET network on CIFAR-10 data, but trained only on odd numbered CIFAR-10 classes. This was trained for 5000 iterations and achieved an accuracy of 78%. The notebook that we used to build this is <a href=\"Reference_CIFAR10-OddClassesTraining.ipynb\">here</a>. Go to the notebook for reference, but running it again may take some time and is not recommended. \n",
    "\n",
    "In this notebook we will build a second model, with the weights from the initial model as the starting point. But, this time, we train on the even numbered classes in the CIFAR-10 data for 2000 iterations.\n",
    "\n",
    "We will show that, the second model that we build in this notebook will achieve similar accuracy with lesser iterations, because it **transfers** some important features from the first model.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try it out\n",
    "\n",
    "\n",
    "Before we begin, it would be good to copy this notebook and rename it with your name at the end, since we don't want multiple people editing the same notebook at the same time, causing reloading issues.\n",
    "\n",
    "#### Imports\n",
    "After that, lets start by importing the necessary packages, mainly learning and dataset. The learning package contains everything related to defining a model learning experiment.\n",
    "The dataset package contains everything related to defining dataset transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.learning.experiment import *\n",
    "from sbrain.dataset.dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Names\n",
    "\n",
    "Most of the **SBrain** artifacts you would create in this notebook like estimators, models and jobs, need to have a human readable unique name which others can look up and possibly reuse or inspect. We provide you a helper function here to make the names unique by appending the username and a timestamp at the end, so that you don't run into DuplicateName error every now and then. You can turn this off by changing the flag should_uniquify to False. Please, put your username as the value for the user_name field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "user_name = \"albin\"\n",
    "\n",
    "def uniquify(name):\n",
    "    import time\n",
    "    should_uniquify = True\n",
    "    if should_uniquify:\n",
    "        return name + user_name + str(time.time()).replace(\".\",\"\")\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer Notebook\n",
    "\n",
    "For an introduction on basic SBrain abstractions like Estimator, LearningJob, Model etc, refer to the ExperimentManagement notebook <a href=\"1_SbrainExperimentManagement.ipynb\">here</a>. In this notebook, we talk only about things not covered in the ExperimentManagement notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Function\n",
    "\n",
    "\n",
    "For most usecases, **SBrain** uses datasets and dataset splits as the data feeding interface for learning. But, in some cases, there may be a need to download some open datasets directly and train on them. For such rare cases, **SBrain** provides another user interface called the input_function. \n",
    "\n",
    "The input_function is a function to be implemented by the user which returns a **tf.data.TFRecordDataset** for train and eval modes. For this, example we plan to use this interface for training. Below given is an example for an input_function, where we download the CIFAR-10 data, load TFRecordDataset from it and filter it for only the odd numbered classes. \n",
    "\n",
    "The input function has these parameters.\n",
    "\n",
    "1. **mode** : The mode that is invoked in. Either **tf.estimator.ModeKeys.TRAIN** or **tf.estimator.ModeKeys.EVAL**\n",
    "2. **batch_size** : The batch size to use, when batching the TFRecordDataset\n",
    "3. **params** : Additional hyper parameters that are passed in at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_even_classes_input_function(mode, batch_size, params):\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import sys\n",
    "    import tarfile\n",
    "    import pickle\n",
    "    CLASS_INDEX_MOD = 1\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32\n",
    "    DEPTH = 3\n",
    "    CIFAR_FILENAME = 'cifar-10-python.tar.gz'\n",
    "    CIFAR_DOWNLOAD_URL = 'https://www.cs.toronto.edu/~kriz/' + CIFAR_FILENAME\n",
    "    CIFAR_LOCAL_FOLDER = 'cifar-10-batches-py'\n",
    "    def download_and_extract(data_dir):\n",
    "        tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir,\n",
    "                                                      CIFAR_DOWNLOAD_URL)\n",
    "        tarfile.open(os.path.join(data_dir, CIFAR_FILENAME),\n",
    "                     'r:gz').extractall(data_dir)\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    def _get_file_names():\n",
    "        file_names = {}\n",
    "        file_names['train'] = ['data_batch_%d' % i for i in range(1, 5)]\n",
    "        file_names['validation'] = ['data_batch_5']\n",
    "        file_names['eval'] = ['test_batch']\n",
    "        return file_names\n",
    "    def read_pickle_from_file(filename):\n",
    "        with tf.gfile.Open(filename, 'rb') as f:\n",
    "            if sys.version_info >= (3, 0):\n",
    "                data_dict = pickle.load(f, encoding='bytes')\n",
    "            else:\n",
    "                data_dict = pickle.load(f)\n",
    "        return data_dict\n",
    "    def convert_to_tfrecord(input_files, output_file):\n",
    "        print('Generating %s' % output_file)\n",
    "        with tf.python_io.TFRecordWriter(output_file) as record_writer:\n",
    "            for input_file in input_files:\n",
    "                data_dict = read_pickle_from_file(input_file)\n",
    "                data = data_dict[b'data']\n",
    "                labels = data_dict[b'labels']\n",
    "                num_entries_in_batch = len(labels)\n",
    "                for i in range(num_entries_in_batch):\n",
    "                    example = tf.train.Example(features=tf.train.Features(\n",
    "                        feature={\n",
    "                            'image': _bytes_feature(data[i].tobytes()),\n",
    "                            'label': _int64_feature(labels[i])\n",
    "                        }))\n",
    "                    record_writer.write(example.SerializeToString())\n",
    "    def setup_cifar10_data(data_dir):\n",
    "        train_dir = os.path.join(data_dir, 'train.tfrecords')\n",
    "        validation_dir = os.path.join(data_dir, 'validation.tfrecords')\n",
    "        eval_dir = os.path.join(data_dir, 'eval.tfrecords')\n",
    "        if os.path.exists(train_dir) and os.path.exists(validation_dir) and os.path.exists(eval_dir):\n",
    "            print(\"Data already present.\")\n",
    "        else:\n",
    "            print('Download from {} and extract. Wait for download complete message..'.format(CIFAR_DOWNLOAD_URL))\n",
    "            download_and_extract(data_dir)\n",
    "            print('Download completed')\n",
    "            file_names = _get_file_names()\n",
    "            input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)\n",
    "            for mode, files in file_names.items():\n",
    "                input_files = [os.path.join(input_dir, f) for f in files]\n",
    "                output_file = os.path.join(data_dir, mode + '.tfrecords')\n",
    "                try:\n",
    "                    os.remove(output_file)\n",
    "                except OSError:\n",
    "                    pass\n",
    "                # Convert to tf.train.Example and write the to TFRecords.\n",
    "                convert_to_tfrecord(input_files, output_file)\n",
    "            print('Done!')\n",
    "    class Cifar10DataSet(object):\n",
    "        def __init__(self, data_dir, subset='train', use_distortion=True):\n",
    "            self.data_dir = data_dir\n",
    "            self.subset = subset\n",
    "            self.use_distortion = use_distortion\n",
    "        def get_filenames(self):\n",
    "            if self.subset in ['train', 'validation', 'eval']:\n",
    "                return [os.path.join(self.data_dir, self.subset + '.tfrecords')]\n",
    "            else:\n",
    "                raise ValueError('Invalid data subset \"%s\"' % self.subset)\n",
    "        def parser(self, serialized_example):\n",
    "            features = tf.parse_single_example(\n",
    "                serialized_example,\n",
    "                features={\n",
    "                    'image': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64),\n",
    "                })\n",
    "            image = tf.decode_raw(features['image'], tf.uint8)\n",
    "            image.set_shape([DEPTH * HEIGHT * WIDTH])\n",
    "\n",
    "            image = tf.cast(\n",
    "                tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]),\n",
    "                tf.float32)\n",
    "            label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "            image = self.preprocess(image)\n",
    "\n",
    "            return ({\"data\": image}, label)\n",
    "\n",
    "        def adjust_labels(self, data, label):\n",
    "            return data, tf.floordiv(label, 2)\n",
    "\n",
    "        def filter_fun(self, data, label):\n",
    "            return tf.equal(tf.mod(label, 2), CLASS_INDEX_MOD)\n",
    "\n",
    "        def get_dataset(self):\n",
    "            \"\"\"Read the images and labels from 'filenames'.\"\"\"\n",
    "            filenames = self.get_filenames()\n",
    "\n",
    "            dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "            # Parse records.\n",
    "            dataset = dataset.map(self.parser).filter(self.filter_fun).map(self.adjust_labels)\n",
    "            return dataset\n",
    "\n",
    "        def preprocess(self, image):\n",
    "            \"\"\"Preprocess a single image in [height, width, depth] layout.\"\"\"\n",
    "            if self.subset == 'train' and self.use_distortion:\n",
    "                # Pad 4 pixels on each dimension of feature map, done in mini-batch\n",
    "                image = tf.image.resize_image_with_crop_or_pad(image, 40, 40)\n",
    "                image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH])\n",
    "                image = tf.image.random_flip_left_right(image)\n",
    "            return image\n",
    "\n",
    "    use_distortion = False\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        subset = 'train'\n",
    "        use_distortion = True\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        subset = 'validation'\n",
    "    else:\n",
    "        subset = 'eval'\n",
    "    data_dir = \"/workspace/shared-dir/sample-notebooks/demo-data/learning/EvenClasses/\"\n",
    "\n",
    "    setup_cifar10_data(data_dir)\n",
    "\n",
    "    dataset = Cifar10DataSet(data_dir, subset, use_distortion).get_dataset()\n",
    "\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model function\n",
    "\n",
    "Here we define the model function for RESNET architecture. This is very similar to the ExperimentManagement notebook <a href=\"1_SbrainExperimentManagement.ipynb\">here</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_model_function(features, labels, mode, params):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    ######################################## Resnet model #########################################\n",
    "\n",
    "    class ResNet(object):\n",
    "        \"\"\"ResNet model.\"\"\"\n",
    "\n",
    "        def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):\n",
    "            \"\"\"ResNet constructor.\n",
    "\n",
    "            Args:\n",
    "              is_training: if build training or inference model.\n",
    "              data_format: the data_format used during computation.\n",
    "                           one of 'channels_first' or 'channels_last'.\n",
    "            \"\"\"\n",
    "            self._batch_norm_decay = batch_norm_decay\n",
    "            self._batch_norm_epsilon = batch_norm_epsilon\n",
    "            self._is_training = is_training\n",
    "            assert data_format in ('channels_first', 'channels_last')\n",
    "            self._data_format = data_format\n",
    "\n",
    "        def forward_pass(self, x):\n",
    "            raise NotImplementedError(\n",
    "                'forward_pass() is implemented in ResNet sub classes')\n",
    "\n",
    "        def _residual_v1(self,\n",
    "                         x,\n",
    "                         kernel_size,\n",
    "                         in_filter,\n",
    "                         out_filter,\n",
    "                         stride,\n",
    "                         activate_before_residual=False):\n",
    "            \"\"\"Residual unit with 2 sub layers, using Plan A for shortcut connection.\"\"\"\n",
    "\n",
    "            del activate_before_residual\n",
    "            with tf.name_scope('residual_v1') as name_scope:\n",
    "                orig_x = x\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, stride)\n",
    "                x = self._batch_norm(x)\n",
    "                x = self._relu(x)\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, 1)\n",
    "                x = self._batch_norm(x)\n",
    "\n",
    "                if in_filter != out_filter:\n",
    "                    orig_x = self._avg_pool(orig_x, stride, stride)\n",
    "                    pad = (out_filter - in_filter) // 2\n",
    "                    if self._data_format == 'channels_first':\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n",
    "                    else:\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n",
    "\n",
    "                x = self._relu(tf.add(x, orig_x))\n",
    "\n",
    "                tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "                return x\n",
    "\n",
    "        def _conv(self, x, kernel_size, filters, strides, is_atrous=False):\n",
    "            \"\"\"Convolution.\"\"\"\n",
    "\n",
    "            padding = 'SAME'\n",
    "            if not is_atrous and strides > 1:\n",
    "                pad = kernel_size - 1\n",
    "                pad_beg = pad // 2\n",
    "                pad_end = pad - pad_beg\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "                else:\n",
    "                    x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n",
    "                padding = 'VALID'\n",
    "            return tf.layers.conv2d(\n",
    "                inputs=x,\n",
    "                kernel_size=kernel_size,\n",
    "                filters=filters,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                use_bias=False,\n",
    "                data_format=self._data_format)\n",
    "\n",
    "        def _batch_norm(self, x):\n",
    "            if self._data_format == 'channels_first':\n",
    "                data_format = 'NCHW'\n",
    "            else:\n",
    "                data_format = 'NHWC'\n",
    "            return tf.contrib.layers.batch_norm(\n",
    "                x,\n",
    "                decay=self._batch_norm_decay,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                epsilon=self._batch_norm_epsilon,\n",
    "                is_training=self._is_training,\n",
    "                fused=True,\n",
    "                data_format=data_format)\n",
    "\n",
    "        def _relu(self, x):\n",
    "            return tf.nn.relu(x)\n",
    "\n",
    "        def _fully_connected(self, x, out_dim):\n",
    "            with tf.name_scope('fully_connected') as name_scope:\n",
    "                x = tf.layers.dense(x, out_dim)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _avg_pool(self, x, pool_size, stride):\n",
    "            with tf.name_scope('avg_pool') as name_scope:\n",
    "                x = tf.layers.average_pooling2d(\n",
    "                    x, pool_size, stride, 'SAME', data_format=self._data_format)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _global_avg_pool(self, x):\n",
    "            with tf.name_scope('global_avg_pool') as name_scope:\n",
    "                assert x.get_shape().ndims == 4\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.reduce_mean(x, [2, 3])\n",
    "                else:\n",
    "                    x = tf.reduce_mean(x, [1, 2])\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "    ####################################### end Resnet base model definition ######################################\n",
    "\n",
    "    ####################################### start cifar resnet subclassing ########################################\n",
    "\n",
    "    class ResNetCifar10(ResNet):\n",
    "        \"\"\"Cifar10 model with ResNetV1 and basic residual block.\"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                     num_layers,\n",
    "                     is_training,\n",
    "                     batch_norm_decay,\n",
    "                     batch_norm_epsilon,\n",
    "                     data_format='channels_first'):\n",
    "            super(ResNetCifar10, self).__init__(\n",
    "                is_training,\n",
    "                data_format,\n",
    "                batch_norm_decay,\n",
    "                batch_norm_epsilon\n",
    "            )\n",
    "            self.n = (num_layers - 2) // 6\n",
    "            # Add one in case label starts with 1. No impact if label starts with 0.\n",
    "            self.num_classes = 5 + 1\n",
    "            self.filters = [16, 16, 32, 64]\n",
    "            self.strides = [1, 2, 2]\n",
    "\n",
    "        def forward_pass(self, x, input_data_format='channels_last'):\n",
    "            \"\"\"Build the core model within the graph.\"\"\"\n",
    "            if self._data_format != input_data_format:\n",
    "                if input_data_format == 'channels_last':\n",
    "                    # Computation requires channels_first.\n",
    "                    x = tf.transpose(x, [0, 3, 1, 2])\n",
    "                else:\n",
    "                    # Computation requires channels_last.\n",
    "                    x = tf.transpose(x, [0, 2, 3, 1])\n",
    "\n",
    "            # Image standardization.\n",
    "            x = x / 128 - 1\n",
    "\n",
    "            x = self._conv(x, 3, 16, 1)\n",
    "            x = self._batch_norm(x)\n",
    "            x = self._relu(x)\n",
    "\n",
    "            # Use basic (non-bottleneck) block and ResNet V1 (post-activation).\n",
    "            res_func = self._residual_v1\n",
    "\n",
    "            # 3 stages of block stacking.\n",
    "            for i in range(3):\n",
    "                with tf.name_scope('stage'):\n",
    "                    for j in range(self.n):\n",
    "                        if j == 0:\n",
    "                            # First block in a stage, filters and strides may change.\n",
    "                            x = res_func(x, 3, self.filters[i], self.filters[i + 1],\n",
    "                                         self.strides[i])\n",
    "                        else:\n",
    "                            # Following blocks in a stage, constant filters and unit stride.\n",
    "                            x = res_func(x, 3, self.filters[i + 1], self.filters[i + 1], 1)\n",
    "\n",
    "            x = self._global_avg_pool(x)\n",
    "            x = self._fully_connected(x, self.num_classes)\n",
    "\n",
    "            return x\n",
    "    ####################################### end cifar resnet subclassing ########################################\n",
    "\n",
    "    ####################################### start loss etc definitions ##########################################\n",
    "\n",
    "    num_layers = 44\n",
    "    batch_norm_decay = 0.997\n",
    "    batch_norm_epsilon = 1e-5\n",
    "    weight_decay = 2e-4\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    model = ResNetCifar10(\n",
    "        num_layers,\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=\"channels_last\")\n",
    "\n",
    "    data = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(\"data\", shape=(32,32,3))])\n",
    "    data = tf.reshape(data, (-1,32,32,3))\n",
    "    logits = model.forward_pass(data, input_data_format='channels_last')\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': tf.argmax(input=logits, axis=1),\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    model_params = tf.trainable_variables()\n",
    "    loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "\n",
    "    ####################################### end loss etc definitions ############################################\n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, axis=1),\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    # TODO tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec( mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "    #FIXME\n",
    "    num_batches_per_epoch = 45000 // 64  # * num_workers)\n",
    "    boundaries = [ num_batches_per_epoch * x for x in np.array([82, 123, 300], dtype=np.int64)]\n",
    "    staged_lr = [learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "    learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(), boundaries, staged_lr)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    # optimizer = tf.train.SyncReplicasOptimizer(optimizer, 4, 4)\n",
    "    # optimizer.make_session_run_hook()\n",
    "\n",
    "    global_step = tf.train.get_global_step()\n",
    "    print(\"Device is\")\n",
    "    print(global_step.device)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.control_dependencies([train_op]):\n",
    "            train_op = tf.Print(global_step, [global_step])\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_chief_hooks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SBrain objects\n",
    "\n",
    "Here we create the SBrain objects necessary to spawn a job based on our earlier definition. More details provided in the other notebook. <a href=\"1_SbrainExperimentManagement.ipynb\">ExperimentManagementNotebook</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator.NewClassificationEstimator(model_fn=cifar_model_function)\n",
    "name = uniquify(\"CIFAR10_even_class_estimator\")\n",
    "estimator = Estimator.create(name, \"Resnet Cifar10 estimator\", estimator)\n",
    "\n",
    "hyper_parameters = HParams(iterations=2000, batch_size=128)\n",
    "rc = RunConfig(no_of_ps=1, no_of_workers=2, run_eval=True, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Checkpoint and  Transfer Learning Configuration\n",
    "\n",
    "Below we look up the model, that was trained on SBrain in the past. This model, as mentioned earlier, was trained on odd numbered CIFAR-10 classes for 5000 iterations. Also, look at the accuracy achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.lookup(\"Cifar10_Odd_Classes_5000_Iterations\")\n",
    "print(model.model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transferring the weights from this model, we get the **ModelCheckpoint** from this model. **ModelCheckpoint** is an SBrain abstraction which represents the checkpoint directory and weights from this model. Checkpoint also gives you access to the \"trainable tensorflow variables\" that were part of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = model.get_model_checkpoint()\n",
    "print(checkpoint.get_all_trainable_vars())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a transfer learning configuration. A transfer learning configuration defines what all variables we want to transfer from the checkpoint. It has the following parameters.\n",
    "\n",
    "1. model_checkpoint - The ModelCheckpoint object to transfer from\n",
    "2. vars_to_load - A regular expression or a list of regular expressions to indicate what all variables to transfer. \n",
    "3. load_only_trainable_vars - Apply the regular expression only to trainable variables. If False, apply to all variables, including global variables.\n",
    "\n",
    "Currently we transfer all the trainable variables, so that all layers initialize the weights to be that of the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_config = TransferLearningConfig(\n",
    "    model_checkpoint=checkpoint, \n",
    "    vars_to_load=\".*\", \n",
    "    load_only_trainable_vars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment.run(experiment_name=uniquify(\"Cifar10_Even_Trial\"),\n",
    "                     description=\"Cifar10 Model\",\n",
    "                     estimator=estimator,\n",
    "                     hyper_parameters=hyper_parameters,\n",
    "                     run_config=rc,\n",
    "                     dataset_version_split=None,\n",
    "                     input_function=only_even_classes_input_function,\n",
    "                     transfer_learning_config=transfer_learning_config)\n",
    "job = experiment.get_single_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get the tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensorboard url\")\n",
    "print(job.get_tensorboard_url())\n",
    "\n",
    "print(job.has_finished())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will wait for the job to finish. It will hang until it finishes. If you want to run something else, interrupt the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally print the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is the job success?? : {}\".format(job.is_success()))\n",
    "print(\"Model metrics..\")\n",
    "print(job.get_model().model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it with the metrics of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.lookup(\"Cifar10_Odd_Classes_5000_Iterations\")\n",
    "print(model.model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within 2000 iterations it has achieved accuracy in the same range as the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}