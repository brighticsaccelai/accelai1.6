{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\" style=\"border-width:4px\">SBrain DataSet APIs Tutorial </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE : * This is a sample notebook. Please make a copy of it for yourself and try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "This tutorial covers the following:\n",
    "- [Create DataSet](#create_dataset)\n",
    "\n",
    "- [Search And Retrieve Existing DataSets](#search_datasets)\n",
    "\n",
    "- [Search And Retrieve DataSet Versions](#search_versions)\n",
    "\n",
    "- [Transforming DataSet Versions To Generate New DataSet Versions](#trans)\n",
    "    - [Defining and Saving Transformations](#define_trans)\n",
    "    - [Testing Set of Transformations on Single Image](#test_trans)\n",
    "    - [Running Transformation Job](#run_trans)\n",
    "    - [Reusing Transformations](#reuse_trans)\n",
    "    \n",
    "- [Creating Training/Test/Validation Splits](#splits)\n",
    "\n",
    "- [Searching And Retrieving DataSet Splits](#search_splits)\n",
    "\n",
    "\n",
    "We are using a subset of the [cifar_10 dataset](https://en.wikipedia.org/wiki/CIFAR-10), which has pictures of animals, airplanes, ships and so on.\n",
    "\n",
    "Let's first import all the packages that you will need during this tutorial.\n",
    "- sbrain.dataset is the SBrain package with abstractions necessary to use SBrain's DataSet functionalities.\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- cv2  i.e. [openCV](https://opencv.org) is a popular computer vision library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.dataset import DataSetImageClassification,DataSetVersion,DataSetSplit\n",
    "from sbrain.dataset import DataSetStatus,JobStatus,DataSetSplitStatus,DataSetVersionStatus\n",
    "from sbrain.dataset import Transformation,TransformationSet\n",
    "import numpy as np\n",
    "import cv2\n",
    "import uuid\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_NOTE_**: \n",
    "Please set the username you used to log into sbrain ui in the following cell.\n",
    "This value will be used to pass in the search apis to limit the results to the assets created\n",
    "by the current user. Its just for illustration purposes. \n",
    "\n",
    "User can search other users assets and reuse them by passing in other user's username in search apis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"admin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_dataset'></a>\n",
    "# _Create DataSet_#            \n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_sbrain.dataset.DataSetImageClassification_** is an abstraction which supports creating and handling of image dataset for classification model training. \n",
    "\n",
    "DataSetImageClassification construtor takes the **_name_** of the parameter as input.\n",
    "\n",
    "**_DataSetImageClassification.create()_** method takes following parameters:\n",
    "\n",
    "- **source_archive_path** : the path to the folder containing the images and labels. \n",
    "- **classes** : [optional] a dict with class names in the dataset as the keys and class ids as values\n",
    "- **collection_date** : date of collection of data in string format **_mm-dd-yyyy_**\n",
    "- **image_iterator** : function returning an iterator to the list of path of images in the archive\n",
    "- **label_iterator** : function returning an iterator. Each element returned by iterator is \n",
    "a tuple (image name, class id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def unique_id():\n",
    "    return str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining classes\n",
    "\n",
    "classes = {\n",
    "                'airplane': 0,\n",
    "                'automobile':1,\n",
    "                'bird': 2,\n",
    "                'cat': 3,\n",
    "                'deer': 4,\n",
    "                'dog': 5,\n",
    "                'frog': 6,\n",
    "                'horse': 7,\n",
    "                'ship': 8,\n",
    "                'truck': 9\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining iterator to get image file paths\n",
    "\n",
    "def iterator_images(data_root_path):\n",
    "    import glob\n",
    "    result = []\n",
    "    files = glob.glob(\"{}/*.*\".format(data_root_path))\n",
    "    return iter(files)\n",
    "\n",
    "# defining iterator to get tuples (image_name, class_id) e.g. (xyz.jpeg,1)\n",
    "def iterator_labels(data_root_path):\n",
    "    import glob\n",
    "    files = glob.glob(\"{}/*.*\".format(data_root_path))\n",
    "    labels = []\n",
    "    classes = {\n",
    "                'airplane': 0,\n",
    "                'automobile':1,\n",
    "                'bird': 2,\n",
    "                'cat': 3,\n",
    "                'deer': 4,\n",
    "                'dog': 5,\n",
    "                'frog': 6,\n",
    "                'horse': 7,\n",
    "                'ship': 8,\n",
    "                'truck': 9\n",
    "            }\n",
    "    for f in files:\n",
    "        img_name =  f.split('/')[-1:][0]\n",
    "        lbl_str = img_name[img_name.index('_')+1:img_name.index('.')]\n",
    "        lbl_id = classes[lbl_str]\n",
    "        labels.append((img_name, lbl_id))    \n",
    "    return iter(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet.create() will return a DataSetExtractionJob object\n",
    "The job object can be used to track the progress of DataSet creation.\n",
    "\n",
    "job.getdataset() will return a DataSet object that's a handle to the new dataset created\n",
    "\n",
    "\n",
    "#### NOTE:\n",
    "**job.cancel()** api can be used any time to cancel the dataset creation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset\n",
    "\n",
    "dataset_name = \"cifar10-small-{}\".format(unique_id())\n",
    "\n",
    "job = DataSetImageClassification(name=dataset_name).create(\n",
    "    description = \"Dataset with subset images from cifar 10 dataset\",\n",
    "    source_archive_path = \"shared-dir/sample-notebooks/demo-data/cifar10_small\",\n",
    "    classes=classes,\n",
    "    collection_date=\"07-25-2018\",\n",
    "    image_iterator=iterator_images,\n",
    "    label_iterator=iterator_labels\n",
    ")\n",
    "\n",
    "#Check Job Status\n",
    "\n",
    "while job.status != JobStatus.COMPLETE.value and job.status != JobStatus.FAILED.value:\n",
    "    clear_output(wait=True)\n",
    "    job = job.get_status()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = job.get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='search_datasets'></a>\n",
    "# _Search And Retrieve Existing DataSets_ \n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_DataSetImageClassification.search()_** api helps to search for existing datasets using \n",
    "- **author** : name of the user who created dataset and/or \n",
    "- **name** : name of the dataset and/or \n",
    "- **description** : keywords in description \n",
    "<br>DataSets which partially match any of the given parameters are returned.\n",
    "\n",
    "**_DataSetImageClassification.lookup()_** can be used to retrieve a dataset object using dataset name. The name needs to match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the images which belong to a user for example\n",
    "# datasets can also be searched by partial names or desciptions\n",
    "\n",
    "DataSetImageClassification.search( author = user_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup returns a DataSetImageClassification object that can be used to search versions. \n",
    "ds = DataSetImageClassification.lookup(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='search_versions'></a>\n",
    "# _Search And Retrieve DataSet Versions_\n",
    "\n",
    "\n",
    "**__DataSetImageClassification.search_versions()__** can be used to lookup and list out all dataset versions derived from the given dataset using\n",
    "- **__'version_author'__** i.e. name of the user who created dataset and/or \n",
    "- **__'version_name'__** i.e. name of the dataset and/or \n",
    "- **__'version_description'__**. \n",
    "<br>DataSet Versions which partially match any of the given parameters are returned.\n",
    "\n",
    "**__DataSetImageClassification.version()__** can be used to retrieve a particular __DataSetVersion__ by name\n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the versions of the dataset which satisfy the given search criteria\n",
    "# this will print out the details of those versions\n",
    "\n",
    "ds.search_versions(version_author = user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSetVersion.search(author= user_name, dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following method will return a DataSetVersion object \n",
    "# which can be used to invoke transformations or creating splits\n",
    "# Version \"v1\" is created by default when dataset is created\n",
    "\n",
    "ds_version = ds.version(\"v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trans'></a>\n",
    "# _Transforming DataSet Versions To Generate New DataSet Versions_\n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define_trans'></a>\n",
    "SBrain provides **_sbrain.dataset.Transformation_** abstraction that can be used to run transformations on a DataSetVersion at scale. \n",
    "\n",
    "## Defining and Saving Transformations\n",
    "Define custom transactions as one or more classes, each of which will inherit from **_Transformation_** class as shown below.\n",
    "\n",
    "<font color=\"red\">IMPORTANT:</font>\n",
    "1. The custom transformation needs to extend the sbrain.dataset.Transformation class\n",
    "2. And provide a 'process' method which takes in and gives out image as numpy array\n",
    "3. NOTE : Right now transformations support only open cv library.\n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, user has written 2 different transformations, one to resize and one to flip the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flip(Transformation):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def process(self, arr_in):\n",
    "        rotated_image = cv2.flip(arr_in, 1)\n",
    "        return rotated_image\n",
    "    \n",
    "    \n",
    "class Resize(Transformation):\n",
    "    def __init__(self, name, **args):\n",
    "        super().__init__(name)\n",
    "        self.height = args[\"height\"]\n",
    "        self.width = args[\"width\"]\n",
    "        \n",
    "\n",
    "    def process(self, arr_in):\n",
    "        resized_img = cv2.resize(arr_in, (self.width, self.height))\n",
    "        return resized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Transformation.create()_** from base class, is used to save an instance of the transformation in the SBrain. Which can be reused by overriding the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and create a transformations\n",
    "# NOTE : unless the create method is called, the transformation is not saved in SBrain system, and can not be used \n",
    "#        transformation jobs\n",
    "flip_transformation_name = \"flip-{}\".format(unique_id())\n",
    "flip = Flip(name=flip_transformation_name).create(author=user_name,\n",
    "                                        description=\"flipping the image\")\n",
    "\n",
    "\n",
    "resize_transformation_name = \"resize-{}\".format(unique_id())\n",
    "resize = Resize(name=resize_transformation_name, height=100, width=100).create(author=user_name,\n",
    "                                        description=\"resizing the image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test_trans'></a>\n",
    "## Testing Set of Transformations on Single Image\n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example we are showing one transformation. In practise there can be muliple such transformations that can be chained by calling dataset_version.transform(transformation_obj_1).transform(transformation_obj_2) and so on. \n",
    "\n",
    "**_TransformationSet.apply_to_file()_** can be used to test the transformations on a single file to see the results, as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = \"../demo-data/cifar10_small/11102_horse.png\"\n",
    "output_img = \"../../11102_horse_transformed-{}.png\".format(unique_id())\n",
    "TransformationSet.apply_to_file(src_path=input_img,\n",
    "                                des_path=output_img,\n",
    "                                transformations_set=[resize,flip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('before')\n",
    "before = io.imread(input_img)\n",
    "plt.imshow(before)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('after')\n",
    "after = io.imread(output_img)\n",
    "plt.imshow(after)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Once the transformation is satisfactorily written and saved, it can be used to run transformation job on the entire dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run_trans'></a>\n",
    "## Running Transformation Job\n",
    "\n",
    "**_DataSetVersion.transform()_** takes a custom transformation object and returns a **__TransformationSet__** object.\n",
    "\n",
    "Multiple transform() calls on **__TransformationSet__** can be used to chain together the required transformations. \n",
    "\n",
    "**_TransformationSet.run()_** deploys the transformation job on cluster. \n",
    "\n",
    "It takes the following args:\n",
    "- **num_workers**: number of workers to parallelize the transformation job\n",
    "- **target_version** : name of the resultant version\n",
    "- **cores**: number of cores to allocate for each worker. \n",
    "- **memory** : memory to be allocated for each worker.\n",
    "\n",
    "It returns:\n",
    " - **_TransformationJob_** object which is the handle to the job running on the cluster, and can be used to check status of the job.\n",
    "\n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>\n",
    "\n",
    "**TransformationJob.get_status()** can be used to retrieve the current status of the job.\n",
    "\n",
    "**TransformationJob.cancel()** can be used to cancel the job.\n",
    "\n",
    "<font color=\"red\">IMPORTANT:</font> Please make sure the DataSetVersion which is being transformed has status 'Complete'. You can see the status in the search result or DataSetVersion.status.\n",
    "\n",
    "\n",
    "#### NOTE: \n",
    "-- with the sample dataset transformation job takes around 2-3 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code launches a transformation job on the cluster \n",
    "#and returns TransformationJob object\n",
    "dataset_new_version_name = \"flip-resized-100-100-{}\".format(unique_id())\n",
    "tj = ds_version.transform(flip).transform(resize).run(target_version=dataset_new_version_name, num_workers=2)\n",
    "\n",
    "# Check job status\n",
    "status = tj.get_status().lower()\n",
    "while status.lower() != 'complete':\n",
    "    clear_output(wait=True)\n",
    "    status = tj.get_status().lower()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the handle to the new version after transformation \n",
    "ds_version_transformed = ds.version( version_name =  dataset_new_version_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a sample of images of versions before and after transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "\n",
    "rows = 8\n",
    "cols = 2\n",
    "plt.figure(1, figsize=(40,50))\n",
    "iter_before = ds_version.get_iterator()\n",
    "iter_after = ds_version_transformed.get_iterator()\n",
    "iter_before.reset_iterator()\n",
    "iter_after.reset_iterator()\n",
    "imgs_before = [img for img,lbl in iter_before.next_batch(8)]\n",
    "imgs_after = [img for img,lbl in iter_after.next_batch(8)] \n",
    "imgs_before = sorted(imgs_before)\n",
    "imgs_after = sorted(imgs_after)\n",
    "idx = 0\n",
    "for i in range(8):\n",
    "    plt.subplot(rows, cols, idx + 1)\n",
    "    plt.title('before')\n",
    "    before = io.imread(imgs_before[i])\n",
    "    # print(before.shape)\n",
    "    plt.imshow(before)\n",
    "    plt.subplot(rows, cols, idx + 2)\n",
    "    h,w,_ = before.shape\n",
    "    ar = h/w\n",
    "    plt.title('after')\n",
    "    after = io.imread(imgs_after[i])\n",
    "    # print(before.shape)\n",
    "    plt.imshow(after) #,aspect=ar)\n",
    "    idx = idx + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reuse_trans'></a>\n",
    "## Reusing Transformations\n",
    "\n",
    "The same transformation object resize, that was initially created with 'height' and 'width' parameters set to 100, can be used to perform another transformation, for example, with just changing height and width to '300'\n",
    "\n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_resize_300_name = \"flip-resize-300-{}\".format(unique_id())\n",
    "tj2 = ds_version.transform(resize.override(height=300,width=300)).run(num_workers=1, target_version=flip_resize_300_name)\n",
    "#Check job status\n",
    "status = tj2.get_status()\n",
    "while status != JobStatus.COMPLETE.value and status != JobStatus.FAILED.value:\n",
    "    clear_output(wait=True)\n",
    "    status = tj2.get_status()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the handle to the new version after transformation \n",
    "ds_version_transformed_300 = ds.version( version_name =  flip_resize_300_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the images again to see the difference when weight=128 and weight=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "\n",
    "rows = 8\n",
    "cols = 2\n",
    "plt.figure(1, figsize=(40,50))\n",
    "imgs_before = [img for img,lbl in ds_version_transformed.get_iterator().next_batch(8)]\n",
    "imgs_after = [img for img,lbl in ds_version_transformed_300.get_iterator().next_batch(8)] \n",
    "imgs_before = sorted(imgs_before)\n",
    "imgs_after = sorted(imgs_after)\n",
    "idx = 0\n",
    "for i in range(8):\n",
    "    plt.subplot(rows, cols, idx + 1)\n",
    "    plt.title('before')\n",
    "    before = io.imread(imgs_before[i])\n",
    "    # print(before.shape)\n",
    "    plt.imshow(before)\n",
    "    plt.subplot(rows, cols, idx + 2)\n",
    "    h,w,_ = before.shape\n",
    "    ar = h/w\n",
    "    plt.title('after')\n",
    "    after = io.imread(imgs_after[i])\n",
    "    # print(before.shape)\n",
    "    plt.imshow(after) #,aspect=ar)\n",
    "    idx = idx + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"splits\"></a>\n",
    "# Creating Training/Test/Validation Splits\n",
    "\n",
    "Use **_DataSetVersion.create_data_split()_** to create train/test/validation splits. \n",
    "\n",
    "It takes:\n",
    "- **split_name** : Name of the split being created.\n",
    "- **split_percentages**: An array of ints specifying the train/test/validation split percentages in the respective order.\n",
    "- **author**: author of the split\n",
    "- **description** : some description about the split\n",
    "\n",
    "<font color=\"red\">IMPORTANT:</font> Please make sure the DataSetVersion which is being used for split has status 'Complete'. You can see the status in the search result or DataSetVersion.status.\n",
    "\n",
    "It returns:\n",
    "    **_DataSetSplitJob_** object\n",
    "    \n",
    "\n",
    "**DataSetSplitJob.get_status()** can be used to check status of dataset split job\n",
    "\n",
    "**DataSetSplitJob.cancel()** can be used to check status of dataset split job\n",
    "\n",
    "\n",
    "#### NOTE: \n",
    "-- with the sample dataset, creating split takes about 1-2 mins\n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split_name = \"split-70-20-10-{}\".format(unique_id())\n",
    "split_job = ds_version_transformed.create_data_split(split_name = dataset_split_name,\n",
    "                                                 split_percentages = [70,20,10],\n",
    "                                                 description = \"split by 70-20-10\")\n",
    "\n",
    "#Check job status\n",
    "while split_job.status != JobStatus.COMPLETE.value and split_job.status != JobStatus.FAILED.value:\n",
    "    clear_output(wait=True)\n",
    "    split_job = split_job.get_status()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSetSplit.create() will return a DataSetSplitJob object\n",
    "The job object can be used to track the progress of DataSetSplit creation.\n",
    "\n",
    "job.get_dataset_split() will return a DataSetSplit object that's a handle to the new dataset split created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = split_job.get_dataset_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='search_splits'></a>\n",
    "\n",
    "# Searching And Retrieving DataSet Splits\n",
    "\n",
    "**DataSetVersion.search_splits()** can be used to search DataSetSplits created from that particular DataSetVersion.\n",
    "Args:\n",
    "- **split_name**: name of the split.\n",
    "- **split_author** : author of the split.\n",
    "- **split_description** : description of the split.\n",
    "\n",
    "**DataSetVersion.split()** which takes split_name as an argument can be used to retrieve **DataSetSplit** with exact name. \n",
    "<div align=\"right\"><a href=\"#top\">BackToTheTop</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all splits for a particular dataset version which match the search criteria\n",
    "ds_version_transformed.search_splits(split_author = user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a split object for a particular dataset version which can be used in training models\n",
    "split_obj = ds_version_transformed.split(split_name = dataset_split_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " NOTE: **_DataSetSplit_** object is used for training models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **_<font color=\"green\">Congratulations !!! You completed the tutorial successfully.</font>_**\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
