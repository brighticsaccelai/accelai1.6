{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\" style=\"border-width:4px\">SBrain Model Training Using Input Function Then Inference Tutorial </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try it out\n",
    "\n",
    "Before we begin, it would be good to copy this notebook and rename it with your name at the end, since we don't want multiple people editing the same notebook at the same time, causing reloading issues.\n",
    "\n",
    "### NOTE : Please try out the [Experiment Management Notebook](../experiment-management/1_SbrainExperimentManagement.ipynb) before this tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.learning.experiment import *\n",
    "from sbrain.dataset.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"admin\"\n",
    "\n",
    "def uniquify(name):\n",
    "    import time\n",
    "    should_uniquify = True\n",
    "    if should_uniquify:\n",
    "        return name + user_name + str(time.time()).replace(\".\",\"\")\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model Using Input Function\n",
    "\n",
    "We are going to use the cifar10 dataset. Its already split into train,eval,predict subsets. Each of the folder contains images in .png format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_function(mode, batch_size, params):\n",
    "    import os,glob\n",
    "    import tensorflow as tf\n",
    "    import sys\n",
    "    import tarfile\n",
    "    import pickle\n",
    "    CLASS_INDEX_MOD = 0\n",
    "\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32\n",
    "    DEPTH = 3\n",
    "\n",
    "    CIFAR_DATASET_PATH_IN_NFS=\"/workspace/shared-dir/sample-notebooks/demo-data/cifar10-input-function\"\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    class Cifar10DataSet(object):\n",
    "        \"\"\"Cifar10 data set.\n",
    "        Described by http://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, data_dir, subset='train'):\n",
    "            self.data_dir = data_dir\n",
    "            self.subset = subset\n",
    "\n",
    "        def get_filenames(self):\n",
    "            if self.subset in ['train', 'eval', 'predict']:\n",
    "                path = os.path.join(CIFAR_DATASET_PATH_IN_NFS, self.subset)\n",
    "                files = glob.glob(\"{}/*.png\".format(path))\n",
    "                return files\n",
    "            else:\n",
    "                raise ValueError('Invalid data subset \"%s\"' % self.subset)\n",
    "    \n",
    "        def parser(self, filename, label):\n",
    "            image_string = tf.read_file(filename)\n",
    "            image_decoded = tf.image.decode_png(image_string, channels=3)\n",
    "            image = tf.cast(image_decoded, tf.float32)\n",
    "            label = tf.cast(label, tf.int32)\n",
    "            return ({\"data\": image}, label)\n",
    "\n",
    "        \n",
    "        def get_dataset(self):\n",
    "            \"\"\"Read the images and labels from 'filenames'.\"\"\"\n",
    "            filenames = self.get_filenames()\n",
    "            labels = [] \n",
    "            classes = {\n",
    "                'airplane': 0,\n",
    "                'automobile':1,\n",
    "                'bird': 2,\n",
    "                'cat': 3,\n",
    "                'deer': 4,\n",
    "                'dog': 5,\n",
    "                'frog': 6,\n",
    "                'horse': 7,\n",
    "                'ship': 8,\n",
    "                'truck': 9\n",
    "            }\n",
    "            \n",
    "            for f in filenames:\n",
    "                img_name =  f.split('/')[-1:][0]\n",
    "                lbl_str = img_name[img_name.index('_')+1:img_name.index('.')]\n",
    "                lbl_id = classes[lbl_str]\n",
    "                labels.append(lbl_id)\n",
    "\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "            dataset = dataset.map(self.parser)\n",
    "            return dataset\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        subset = 'train'\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        subset = 'eval'\n",
    "    else:\n",
    "        subset = 'predict'\n",
    "   \n",
    "\n",
    "    dataset = Cifar10DataSet(CIFAR_DATASET_PATH_IN_NFS, subset).get_dataset()\n",
    "\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_model_function(features, labels, mode, params):\n",
    "    ## Importing relevant packages\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    ########## Defining ResNet structure as a class. #############\n",
    "\n",
    "    class ResNet(object):\n",
    "        \"\"\"ResNet model.\"\"\"\n",
    "\n",
    "        def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):\n",
    "            \"\"\"ResNet constructor.\n",
    "\n",
    "            Args:\n",
    "              is_training: if build training or inference model.\n",
    "              data_format: the data_format used during computation.\n",
    "                           one of 'channels_first' or 'channels_last'.\n",
    "            \"\"\"\n",
    "            self._batch_norm_decay = batch_norm_decay\n",
    "            self._batch_norm_epsilon = batch_norm_epsilon\n",
    "            self._is_training = is_training\n",
    "            assert data_format in ('channels_first', 'channels_last')\n",
    "            self._data_format = data_format\n",
    "\n",
    "        def forward_pass(self, x):\n",
    "            raise NotImplementedError(\n",
    "                'forward_pass() is implemented in ResNet sub classes')\n",
    "\n",
    "        def _residual_v1(self,\n",
    "                         x,\n",
    "                         kernel_size,\n",
    "                         in_filter,\n",
    "                         out_filter,\n",
    "                         stride,\n",
    "                         activate_before_residual=False):\n",
    "            \"\"\"Residual unit with 2 sub layers, using Plan A for shortcut connection.\"\"\"\n",
    "\n",
    "            del activate_before_residual\n",
    "            with tf.name_scope('residual_v1') as name_scope:\n",
    "                orig_x = x\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, stride)\n",
    "                x = self._batch_norm(x)\n",
    "                x = self._relu(x)\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, 1)\n",
    "                x = self._batch_norm(x)\n",
    "\n",
    "                if in_filter != out_filter:\n",
    "                    orig_x = self._avg_pool(orig_x, stride, stride)\n",
    "                    pad = (out_filter - in_filter) // 2\n",
    "                    if self._data_format == 'channels_first':\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n",
    "                    else:\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n",
    "\n",
    "                x = self._relu(tf.add(x, orig_x))\n",
    "\n",
    "                tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "                return x\n",
    "\n",
    "        def _conv(self, x, kernel_size, filters, strides, is_atrous=False):\n",
    "            \"\"\"Convolution.\"\"\"\n",
    "\n",
    "            padding = 'SAME'\n",
    "            if not is_atrous and strides > 1:\n",
    "                pad = kernel_size - 1\n",
    "                pad_beg = pad // 2\n",
    "                pad_end = pad - pad_beg\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "                else:\n",
    "                    x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n",
    "                padding = 'VALID'\n",
    "            return tf.layers.conv2d(\n",
    "                inputs=x,\n",
    "                kernel_size=kernel_size,\n",
    "                filters=filters,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                use_bias=False,\n",
    "                data_format=self._data_format)\n",
    "\n",
    "        def _batch_norm(self, x):\n",
    "            if self._data_format == 'channels_first':\n",
    "                data_format = 'NCHW'\n",
    "            else:\n",
    "                data_format = 'NHWC'\n",
    "            return tf.contrib.layers.batch_norm(\n",
    "                x,\n",
    "                decay=self._batch_norm_decay,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                epsilon=self._batch_norm_epsilon,\n",
    "                is_training=self._is_training,\n",
    "                fused=True,\n",
    "                data_format=data_format)\n",
    "\n",
    "        def _relu(self, x):\n",
    "            return tf.nn.relu(x)\n",
    "\n",
    "        def _fully_connected(self, x, out_dim):\n",
    "            with tf.name_scope('fully_connected') as name_scope:\n",
    "                x = tf.layers.dense(x, out_dim)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _avg_pool(self, x, pool_size, stride):\n",
    "            with tf.name_scope('avg_pool') as name_scope:\n",
    "                x = tf.layers.average_pooling2d(\n",
    "                    x, pool_size, stride, 'SAME', data_format=self._data_format)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _global_avg_pool(self, x):\n",
    "            with tf.name_scope('global_avg_pool') as name_scope:\n",
    "                assert x.get_shape().ndims == 4\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.reduce_mean(x, [2, 3])\n",
    "                else:\n",
    "                    x = tf.reduce_mean(x, [1, 2])\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "    ########## End ResNet class #############\n",
    "\n",
    "    ####### Subclassing ResNet specific to CIFAR-10 ###########\n",
    "\n",
    "    class ResNetCifar10(ResNet):\n",
    "        \"\"\"Cifar10 model with ResNetV1 and basic residual block.\"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                     num_layers,\n",
    "                     is_training,\n",
    "                     batch_norm_decay,\n",
    "                     batch_norm_epsilon,\n",
    "                     data_format='channels_first'):\n",
    "            super(ResNetCifar10, self).__init__(\n",
    "                is_training,\n",
    "                data_format,\n",
    "                batch_norm_decay,\n",
    "                batch_norm_epsilon\n",
    "            )\n",
    "            self.n = (num_layers - 2) // 6\n",
    "            # Add one in case label starts with 1. No impact if label starts with 0.\n",
    "            self.num_classes = 10\n",
    "            self.filters = [16, 16, 32, 64]\n",
    "            self.strides = [1, 2, 2]\n",
    "\n",
    "        def forward_pass(self, x, input_data_format='channels_last'):\n",
    "            \"\"\"Build the core model within the graph.\"\"\"\n",
    "            if self._data_format != input_data_format:\n",
    "                if input_data_format == 'channels_last':\n",
    "                    # Computation requires channels_first.\n",
    "                    x = tf.transpose(x, [0, 3, 1, 2])\n",
    "                else:\n",
    "                    # Computation requires channels_last.\n",
    "                    x = tf.transpose(x, [0, 2, 3, 1])\n",
    "\n",
    "            # Image standardization.\n",
    "            x = x / 128 - 1\n",
    "\n",
    "            x = self._conv(x, 3, 16, 1)\n",
    "            x = self._batch_norm(x)\n",
    "            x = self._relu(x)\n",
    "\n",
    "            # Use basic (non-bottleneck) block and ResNet V1 (post-activation).\n",
    "            res_func = self._residual_v1\n",
    "\n",
    "            # 3 stages of block stacking.\n",
    "            for i in range(3):\n",
    "                with tf.name_scope('stage'):\n",
    "                    for j in range(self.n):\n",
    "                        if j == 0:\n",
    "                            # First block in a stage, filters and strides may change.\n",
    "                            x = res_func(x, 3, self.filters[i], self.filters[i + 1],\n",
    "                                         self.strides[i])\n",
    "                        else:\n",
    "                            # Following blocks in a stage, constant filters and unit stride.\n",
    "                            x = res_func(x, 3, self.filters[i + 1], self.filters[i + 1], 1)\n",
    "\n",
    "            x = self._global_avg_pool(x)\n",
    "            x = self._fully_connected(x, self.num_classes)\n",
    "\n",
    "            return x\n",
    "    ####### End ResNetCifar10 class ###########\n",
    "\n",
    "    ######### Here we define all the hyperparameters, network, loss, optimzier and training operations ##################\n",
    "\n",
    "    ## Hyperparams\n",
    "    num_layers = 44\n",
    "\n",
    "    # batch_norm_decay = 0.997\n",
    "    batch_norm_decay = params[\"batch_norm_decay\"]\n",
    "    batch_norm_epsilon = 1e-5\n",
    "    # weight_decay = 2e-4\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    ## Neural network architecture\n",
    "    model = ResNetCifar10(\n",
    "        num_layers,\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=\"channels_last\")\n",
    "\n",
    "    data = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(\"data\", shape=(32,32,3))])\n",
    "    data = tf.reshape(data, (-1,32,32,3))\n",
    "    logits = model.forward_pass(data, input_data_format='channels_last')\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': tf.argmax(input=logits, axis=1),\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    ## Defining Loss\n",
    "#     labels = tf.string_to_number(labels,out_type=tf.int32)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    model_params = tf.trainable_variables()\n",
    "    loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "\n",
    "    ## Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, axis=1),\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec( mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    ## Create optimizer\n",
    "    num_batches_per_epoch = 45000 // 64\n",
    "    boundaries = [ num_batches_per_epoch * x for x in np.array([82, 123, 300], dtype=np.int64)]\n",
    "    staged_lr = [learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "    learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(), boundaries, staged_lr)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    ## Create global step and training operation\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    ## Some print operations for better logging.\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.control_dependencies([train_op]):\n",
    "            train_op = tf.Print(predictions, [predictions, tf.shape(predictions), \"predictions\"], summarize=32)\n",
    "            train_op = tf.Print(global_step, [global_step])\n",
    "\n",
    "    ## Return Estimator Spec with loss and training operation\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_chief_hooks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model using SBrain Estimator\n",
    "\n",
    "The model function we defined above captures the structure of the network, loss and training operation. The next step is to tie this up to other **SBrain** abstractions.\n",
    "\n",
    "Here, we define a new **SBrain** classification estimator, passing in the same model_function that we defined earlier. This gives us an **SBrain** object which packages your model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_estimator = Estimator.NewClassificationEstimator(model_fn=cifar_model_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save this estimator as an asset in the **SBrain** environment, with a name we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = uniquify(\"ResnetCifar10InputFunc\")\n",
    "saved_estimator = Estimator.create(estimator_name=name,\n",
    "                                   description=\"ResNet Cifar10 estimator trial\",\n",
    "                                   estimator_obj=classification_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching Estimators\n",
    "Also you can search for specific ones using name/description as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resnet_estimators = Estimator.search(estimator_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfig(no_of_ps=1, no_of_workers=4, summary_save_frequency=10, run_eval=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = HParams(iterations=50,\n",
    "                       batch_size=8,\n",
    "                       batch_norm_decay=0.9,\n",
    "                       batch_norm_epsilon=1e-5,\n",
    "                       weight_decay=2e-4,\n",
    "                       learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = uniquify(\"Resnet_CIFAR10_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment.run(experiment_name=experiment_name,\n",
    "                                description=\"ResNet Model trained on Cifar10 data\",\n",
    "                                estimator=saved_estimator,\n",
    "                                hyper_parameters=hyper_parameters,\n",
    "                                run_config=run_config,\n",
    "                                dataset_version_split=None,\n",
    "                                input_function=input_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>At this point, you have started an experiment run. It will start multiple training jobs in **SBrain**. It is currently executing in the cluster. The Experiment object is your handle to the training job/s that are currently running in the cluster which are part of the particular experiment. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.has_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below methods will report the statuses of number of jobs under this experiment. List jobs will list all jobs under this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.report_status()\n",
    "experiment.list_jobs()\n",
    "experiment.wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = LearningJob.lookup(experiment_name)\n",
    "model = job.get_model()\n",
    "print(model.model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching Model Inference Endpoint and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.serving.model_service_endpoint import *\n",
    "\n",
    "end_point_name=\"ep_{}\".format(model.model_name)\n",
    "mep = ModelEndpoint.create(model=model, \n",
    "                           endpoint_name=end_point_name,\n",
    "                           description=end_point_name, \n",
    "                           number_of_service_replicas=1, \n",
    "                           gpu_required=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mep.search(endpoint_name=end_point_name)\n",
    "mep.internal_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def img_to_array(img_path):\n",
    "    img = Image.open(img_path,\"r\")\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "root_dir = \"../demo-data/cifar10-input-function/predict\"\n",
    "files = glob.glob(\"{}/*.png\".format(root_dir))\n",
    "files = files[:100]\n",
    "images_numpy_arr = []\n",
    "for f in files:\n",
    "    images_numpy_arr.append(img_to_array(f))\n",
    "\n",
    "features_dict = {}\n",
    "features_dict['features'] = images_numpy_arr\n",
    "result = mep.predict(features_dict)\n",
    "# print(result)\n",
    "predicted = result[0]['class_ids']\n",
    "classes = {\n",
    "                'airplane': 0,\n",
    "                'automobile':1,\n",
    "                'bird': 2,\n",
    "                'cat': 3,\n",
    "                'deer': 4,\n",
    "                'dog': 5,\n",
    "                'frog': 6,\n",
    "                'horse': 7,\n",
    "                'ship': 8,\n",
    "                'truck': 9\n",
    "            }\n",
    "\n",
    "correct=0\n",
    "for i in range(100):\n",
    "    img_name = files[i].split('/')[-1:][0]\n",
    "    lbl_str = img_name[img_name.index('_')+1:img_name.index('.')]\n",
    "    orig_lbl = classes[lbl_str]\n",
    "    if orig_lbl == predicted[i]:\n",
    "        correct += 1    \n",
    "print(\"Accuracy {} %\".format(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
