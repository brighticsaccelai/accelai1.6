{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\" style=\"border-width:4px\">SBrain Model Training Using Input Function Then Inference Tutorial </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try it out\n",
    "\n",
    "Before we begin, it would be good to copy this notebook and rename it with your name at the end, since we don't want multiple people editing the same notebook at the same time, causing reloading issues.\n",
    "\n",
    "### NOTE : Please try out the [Experiment Management Notebook](../experiment-management/1_SbrainExperimentManagement.ipynb) before this tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.learning.experiment import *\n",
    "from sbrain.dataset.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"admin\"\n",
    "\n",
    "def uniquify(name):\n",
    "    import time\n",
    "    should_uniquify = True\n",
    "    if should_uniquify:\n",
    "        return name + user_name + str(time.time()).replace(\".\",\"\")\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model Using Input Function\n",
    "\n",
    "We are going to use the cifar10 dataset. Its already split into train,eval,predict subsets. Each of the folder contains images in .png format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_function(mode, batch_size, params):\n",
    "    import os,glob\n",
    "    import tensorflow as tf\n",
    "    import sys\n",
    "    import tarfile\n",
    "    import pickle\n",
    "    CLASS_INDEX_MOD = 0\n",
    "\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32\n",
    "    DEPTH = 3\n",
    "\n",
    "    CIFAR_DATASET_PATH_IN_NFS=\"/workspace/shared-dir/sample-notebooks/demo-data/cifar10-input-function\"\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    class Cifar10DataSet(object):\n",
    "        \"\"\"Cifar10 data set.\n",
    "        Described by http://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, data_dir, subset='train'):\n",
    "            self.data_dir = data_dir\n",
    "            self.subset = subset\n",
    "\n",
    "        def get_filenames(self):\n",
    "            if self.subset in ['train', 'eval', 'predict']:\n",
    "                path = os.path.join(CIFAR_DATASET_PATH_IN_NFS, self.subset)\n",
    "                files = glob.glob(\"{}/*.png\".format(path))\n",
    "                return files\n",
    "            else:\n",
    "                raise ValueError('Invalid data subset \"%s\"' % self.subset)\n",
    "    \n",
    "        def parser(self, filename, label):\n",
    "            image_string = tf.read_file(filename)\n",
    "            image_decoded = tf.image.decode_png(image_string, channels=3)\n",
    "            image = tf.cast(image_decoded, tf.float32)\n",
    "            label = tf.cast(label, tf.int32)\n",
    "            return ({\"data\": image}, label)\n",
    "\n",
    "        \n",
    "        def get_dataset(self):\n",
    "            \"\"\"Read the images and labels from 'filenames'.\"\"\"\n",
    "            filenames = self.get_filenames()\n",
    "            labels = [] \n",
    "            classes = {\n",
    "                'airplane': 0,\n",
    "                'automobile':1,\n",
    "                'bird': 2,\n",
    "                'cat': 3,\n",
    "                'deer': 4,\n",
    "                'dog': 5,\n",
    "                'frog': 6,\n",
    "                'horse': 7,\n",
    "                'ship': 8,\n",
    "                'truck': 9\n",
    "            }\n",
    "            \n",
    "            for f in filenames:\n",
    "                img_name =  f.split('/')[-1:][0]\n",
    "                lbl_str = img_name[img_name.index('_')+1:img_name.index('.')]\n",
    "                lbl_id = classes[lbl_str]\n",
    "                labels.append(lbl_id)\n",
    "\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "            dataset = dataset.map(self.parser)\n",
    "            return dataset\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        subset = 'train'\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        subset = 'eval'\n",
    "    else:\n",
    "        subset = 'predict'\n",
    "   \n",
    "\n",
    "    dataset = Cifar10DataSet(CIFAR_DATASET_PATH_IN_NFS, subset).get_dataset()\n",
    "\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_model_function(features, labels, mode, params):\n",
    "    ## Importing relevant packages\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    ########## Defining ResNet structure as a class. #############\n",
    "\n",
    "    class ResNet(object):\n",
    "        \"\"\"ResNet model.\"\"\"\n",
    "\n",
    "        def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):\n",
    "            \"\"\"ResNet constructor.\n",
    "\n",
    "            Args:\n",
    "              is_training: if build training or inference model.\n",
    "              data_format: the data_format used during computation.\n",
    "                           one of 'channels_first' or 'channels_last'.\n",
    "            \"\"\"\n",
    "            self._batch_norm_decay = batch_norm_decay\n",
    "            self._batch_norm_epsilon = batch_norm_epsilon\n",
    "            self._is_training = is_training\n",
    "            assert data_format in ('channels_first', 'channels_last')\n",
    "            self._data_format = data_format\n",
    "\n",
    "        def forward_pass(self, x):\n",
    "            raise NotImplementedError(\n",
    "                'forward_pass() is implemented in ResNet sub classes')\n",
    "\n",
    "        def _residual_v1(self,\n",
    "                         x,\n",
    "                         kernel_size,\n",
    "                         in_filter,\n",
    "                         out_filter,\n",
    "                         stride,\n",
    "                         activate_before_residual=False):\n",
    "            \"\"\"Residual unit with 2 sub layers, using Plan A for shortcut connection.\"\"\"\n",
    "\n",
    "            del activate_before_residual\n",
    "            with tf.name_scope('residual_v1') as name_scope:\n",
    "                orig_x = x\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, stride)\n",
    "                x = self._batch_norm(x)\n",
    "                x = self._relu(x)\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, 1)\n",
    "                x = self._batch_norm(x)\n",
    "\n",
    "                if in_filter != out_filter:\n",
    "                    orig_x = self._avg_pool(orig_x, stride, stride)\n",
    "                    pad = (out_filter - in_filter) // 2\n",
    "                    if self._data_format == 'channels_first':\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n",
    "                    else:\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n",
    "\n",
    "                x = self._relu(tf.add(x, orig_x))\n",
    "\n",
    "                tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "                return x\n",
    "\n",
    "        def _conv(self, x, kernel_size, filters, strides, is_atrous=False):\n",
    "            \"\"\"Convolution.\"\"\"\n",
    "\n",
    "            padding = 'SAME'\n",
    "            if not is_atrous and strides > 1:\n",
    "                pad = kernel_size - 1\n",
    "                pad_beg = pad // 2\n",
    "                pad_end = pad - pad_beg\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "                else:\n",
    "                    x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n",
    "                padding = 'VALID'\n",
    "            return tf.layers.conv2d(\n",
    "                inputs=x,\n",
    "                kernel_size=kernel_size,\n",
    "                filters=filters,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                use_bias=False,\n",
    "                data_format=self._data_format)\n",
    "\n",
    "        def _batch_norm(self, x):\n",
    "            if self._data_format == 'channels_first':\n",
    "                data_format = 'NCHW'\n",
    "            else:\n",
    "                data_format = 'NHWC'\n",
    "            return tf.contrib.layers.batch_norm(\n",
    "                x,\n",
    "                decay=self._batch_norm_decay,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                epsilon=self._batch_norm_epsilon,\n",
    "                is_training=self._is_training,\n",
    "                fused=True,\n",
    "                data_format=data_format)\n",
    "\n",
    "        def _relu(self, x):\n",
    "            return tf.nn.relu(x)\n",
    "\n",
    "        def _fully_connected(self, x, out_dim):\n",
    "            with tf.name_scope('fully_connected') as name_scope:\n",
    "                x = tf.layers.dense(x, out_dim)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _avg_pool(self, x, pool_size, stride):\n",
    "            with tf.name_scope('avg_pool') as name_scope:\n",
    "                x = tf.layers.average_pooling2d(\n",
    "                    x, pool_size, stride, 'SAME', data_format=self._data_format)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _global_avg_pool(self, x):\n",
    "            with tf.name_scope('global_avg_pool') as name_scope:\n",
    "                assert x.get_shape().ndims == 4\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.reduce_mean(x, [2, 3])\n",
    "                else:\n",
    "                    x = tf.reduce_mean(x, [1, 2])\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "    ########## End ResNet class #############\n",
    "\n",
    "    ####### Subclassing ResNet specific to CIFAR-10 ###########\n",
    "\n",
    "    class ResNetCifar10(ResNet):\n",
    "        \"\"\"Cifar10 model with ResNetV1 and basic residual block.\"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                     num_layers,\n",
    "                     is_training,\n",
    "                     batch_norm_decay,\n",
    "                     batch_norm_epsilon,\n",
    "                     data_format='channels_first'):\n",
    "            super(ResNetCifar10, self).__init__(\n",
    "                is_training,\n",
    "                data_format,\n",
    "                batch_norm_decay,\n",
    "                batch_norm_epsilon\n",
    "            )\n",
    "            self.n = (num_layers - 2) // 6\n",
    "            # Add one in case label starts with 1. No impact if label starts with 0.\n",
    "            self.num_classes = 10\n",
    "            self.filters = [16, 16, 32, 64]\n",
    "            self.strides = [1, 2, 2]\n",
    "\n",
    "        def forward_pass(self, x, input_data_format='channels_last'):\n",
    "            \"\"\"Build the core model within the graph.\"\"\"\n",
    "            if self._data_format != input_data_format:\n",
    "                if input_data_format == 'channels_last':\n",
    "                    # Computation requires channels_first.\n",
    "                    x = tf.transpose(x, [0, 3, 1, 2])\n",
    "                else:\n",
    "                    # Computation requires channels_last.\n",
    "                    x = tf.transpose(x, [0, 2, 3, 1])\n",
    "\n",
    "            # Image standardization.\n",
    "            x = x / 128 - 1\n",
    "\n",
    "            x = self._conv(x, 3, 16, 1)\n",
    "            x = self._batch_norm(x)\n",
    "            x = self._relu(x)\n",
    "\n",
    "            # Use basic (non-bottleneck) block and ResNet V1 (post-activation).\n",
    "            res_func = self._residual_v1\n",
    "\n",
    "            # 3 stages of block stacking.\n",
    "            for i in range(3):\n",
    "                with tf.name_scope('stage'):\n",
    "                    for j in range(self.n):\n",
    "                        if j == 0:\n",
    "                            # First block in a stage, filters and strides may change.\n",
    "                            x = res_func(x, 3, self.filters[i], self.filters[i + 1],\n",
    "                                         self.strides[i])\n",
    "                        else:\n",
    "                            # Following blocks in a stage, constant filters and unit stride.\n",
    "                            x = res_func(x, 3, self.filters[i + 1], self.filters[i + 1], 1)\n",
    "\n",
    "            x = self._global_avg_pool(x)\n",
    "            x = self._fully_connected(x, self.num_classes)\n",
    "\n",
    "            return x\n",
    "    ####### End ResNetCifar10 class ###########\n",
    "\n",
    "    ######### Here we define all the hyperparameters, network, loss, optimzier and training operations ##################\n",
    "\n",
    "    ## Hyperparams\n",
    "    num_layers = 44\n",
    "\n",
    "    # batch_norm_decay = 0.997\n",
    "    batch_norm_decay = params[\"batch_norm_decay\"]\n",
    "    batch_norm_epsilon = 1e-5\n",
    "    # weight_decay = 2e-4\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    ## Neural network architecture\n",
    "    model = ResNetCifar10(\n",
    "        num_layers,\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=\"channels_last\")\n",
    "\n",
    "    data = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(\"data\", shape=(32,32,3))])\n",
    "    data = tf.reshape(data, (-1,32,32,3))\n",
    "    logits = model.forward_pass(data, input_data_format='channels_last')\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': tf.argmax(input=logits, axis=1),\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    ## Defining Loss\n",
    "#     labels = tf.string_to_number(labels,out_type=tf.int32)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    model_params = tf.trainable_variables()\n",
    "    loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "\n",
    "    ## Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, axis=1),\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec( mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    ## Create optimizer\n",
    "    num_batches_per_epoch = 45000 // 64\n",
    "    boundaries = [ num_batches_per_epoch * x for x in np.array([82, 123, 300], dtype=np.int64)]\n",
    "    staged_lr = [learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "    learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(), boundaries, staged_lr)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    ## Create global step and training operation\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    ## Some print operations for better logging.\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.control_dependencies([train_op]):\n",
    "            train_op = tf.Print(predictions, [predictions, tf.shape(predictions), \"predictions\"], summarize=32)\n",
    "            train_op = tf.Print(global_step, [global_step])\n",
    "\n",
    "    ## Return Estimator Spec with loss and training operation\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_chief_hooks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model using SBrain Estimator\n",
    "\n",
    "The model function we defined above captures the structure of the network, loss and training operation. The next step is to tie this up to other **SBrain** abstractions.\n",
    "\n",
    "Here, we define a new **SBrain** classification estimator, passing in the same model_function that we defined earlier. This gives us an **SBrain** object which packages your model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_estimator = Estimator.NewClassificationEstimator(model_fn=cifar_model_function)\n",
    "name = uniquify(\"ResnetCifar10InputFunc\")\n",
    "saved_estimator = Estimator.create(estimator_name=name,\n",
    "                                   description=\"ResNet Cifar10 estimator trial\",\n",
    "                                   estimator_obj=classification_estimator)\n",
    "\n",
    "all_resnet_estimators = Estimator.search(estimator_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfig(no_of_ps=1, no_of_workers=4, summary_save_frequency=10, run_eval=False, use_gpu=False)\n",
    "hyper_parameters = HParams(iterations=50,\n",
    "                       batch_size=8,\n",
    "                       batch_norm_decay=0.9,\n",
    "                       batch_norm_epsilon=1e-5,\n",
    "                       weight_decay=2e-4,\n",
    "                       learning_rate=0.1)\n",
    "\n",
    "experiment_name1 = uniquify(\"Resnet_CIFAR10_model\")\n",
    "\n",
    "experiment1 = Experiment.run(experiment_name=experiment_name1,\n",
    "                                description=\"ResNet Model trained on Cifar10 data\",\n",
    "                                estimator=saved_estimator,\n",
    "                                hyper_parameters=hyper_parameters,\n",
    "                                run_config=run_config,\n",
    "                                dataset_version_split=None,\n",
    "                                input_function=input_function)\n",
    "\n",
    "experiment1.list_jobs()\n",
    "experiment1.wait_until_finish()\n",
    "experiment1.report_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = LearningJob.lookup(experiment_name1)\n",
    "model1 = job.get_model()\n",
    "print(model1.model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using the built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.serving.model_service_endpoint import *\n",
    "\n",
    "end_point_name1=\"ep_{}\".format(model1.model_name)\n",
    "mep1 = ModelEndpoint.create(model=model1, \n",
    "                           endpoint_name=end_point_name1,\n",
    "                           description=end_point_name1, \n",
    "                           number_of_service_replicas=1, \n",
    "                           gpu_required=False)\n",
    "\n",
    "mep1.search(endpoint_name=end_point_name1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import base64\n",
    "def encode_img(img_path):\n",
    "    with open(img_path, \"rb\") as imageFile:\n",
    "        str = base64.b64encode(imageFile.read())\n",
    "        return str.decode(\"utf-8\")\n",
    "    \n",
    "with open(\"prediction_input.csv\", \"r\") as f:\n",
    "    records = f.readlines()\n",
    "\n",
    "records = list(map(lambda x: x.strip().split(\",\"), records))\n",
    "encoded_imgs = [encode_img(records[0][0])]\n",
    "        \n",
    "features_dict = {}\n",
    "features_dict['features'] = encoded_imgs\n",
    "result = mep1.predict(features_dict)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Model Checkpoint and Transfer Learning With Changed Predict Outputs In Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = model1.get_model_checkpoint()\n",
    "transfer_learning_config = TransferLearningConfig(\n",
    "    model_checkpoint=checkpoint, \n",
    "    vars_to_load=\".*\", \n",
    "    load_only_trainable_vars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_model_function_only_logits_output(features, labels, mode, params):\n",
    "    ## Importing relevant packages\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    ########## Defining ResNet structure as a class. #############\n",
    "\n",
    "    class ResNet(object):\n",
    "        \"\"\"ResNet model.\"\"\"\n",
    "\n",
    "        def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):\n",
    "            \"\"\"ResNet constructor.\n",
    "\n",
    "            Args:\n",
    "              is_training: if build training or inference model.\n",
    "              data_format: the data_format used during computation.\n",
    "                           one of 'channels_first' or 'channels_last'.\n",
    "            \"\"\"\n",
    "            self._batch_norm_decay = batch_norm_decay\n",
    "            self._batch_norm_epsilon = batch_norm_epsilon\n",
    "            self._is_training = is_training\n",
    "            assert data_format in ('channels_first', 'channels_last')\n",
    "            self._data_format = data_format\n",
    "\n",
    "        def forward_pass(self, x):\n",
    "            raise NotImplementedError(\n",
    "                'forward_pass() is implemented in ResNet sub classes')\n",
    "\n",
    "        def _residual_v1(self,\n",
    "                         x,\n",
    "                         kernel_size,\n",
    "                         in_filter,\n",
    "                         out_filter,\n",
    "                         stride,\n",
    "                         activate_before_residual=False):\n",
    "            \"\"\"Residual unit with 2 sub layers, using Plan A for shortcut connection.\"\"\"\n",
    "\n",
    "            del activate_before_residual\n",
    "            with tf.name_scope('residual_v1') as name_scope:\n",
    "                orig_x = x\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, stride)\n",
    "                x = self._batch_norm(x)\n",
    "                x = self._relu(x)\n",
    "\n",
    "                x = self._conv(x, kernel_size, out_filter, 1)\n",
    "                x = self._batch_norm(x)\n",
    "\n",
    "                if in_filter != out_filter:\n",
    "                    orig_x = self._avg_pool(orig_x, stride, stride)\n",
    "                    pad = (out_filter - in_filter) // 2\n",
    "                    if self._data_format == 'channels_first':\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n",
    "                    else:\n",
    "                        orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n",
    "\n",
    "                x = self._relu(tf.add(x, orig_x))\n",
    "\n",
    "                tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "                return x\n",
    "\n",
    "        def _conv(self, x, kernel_size, filters, strides, is_atrous=False):\n",
    "            \"\"\"Convolution.\"\"\"\n",
    "\n",
    "            padding = 'SAME'\n",
    "            if not is_atrous and strides > 1:\n",
    "                pad = kernel_size - 1\n",
    "                pad_beg = pad // 2\n",
    "                pad_end = pad - pad_beg\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "                else:\n",
    "                    x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n",
    "                padding = 'VALID'\n",
    "            return tf.layers.conv2d(\n",
    "                inputs=x,\n",
    "                kernel_size=kernel_size,\n",
    "                filters=filters,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                use_bias=False,\n",
    "                data_format=self._data_format)\n",
    "\n",
    "        def _batch_norm(self, x):\n",
    "            if self._data_format == 'channels_first':\n",
    "                data_format = 'NCHW'\n",
    "            else:\n",
    "                data_format = 'NHWC'\n",
    "            return tf.contrib.layers.batch_norm(\n",
    "                x,\n",
    "                decay=self._batch_norm_decay,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                epsilon=self._batch_norm_epsilon,\n",
    "                is_training=self._is_training,\n",
    "                fused=True,\n",
    "                data_format=data_format)\n",
    "\n",
    "        def _relu(self, x):\n",
    "            return tf.nn.relu(x)\n",
    "\n",
    "        def _fully_connected(self, x, out_dim):\n",
    "            with tf.name_scope('fully_connected') as name_scope:\n",
    "                x = tf.layers.dense(x, out_dim)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _avg_pool(self, x, pool_size, stride):\n",
    "            with tf.name_scope('avg_pool') as name_scope:\n",
    "                x = tf.layers.average_pooling2d(\n",
    "                    x, pool_size, stride, 'SAME', data_format=self._data_format)\n",
    "\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "        def _global_avg_pool(self, x):\n",
    "            with tf.name_scope('global_avg_pool') as name_scope:\n",
    "                assert x.get_shape().ndims == 4\n",
    "                if self._data_format == 'channels_first':\n",
    "                    x = tf.reduce_mean(x, [2, 3])\n",
    "                else:\n",
    "                    x = tf.reduce_mean(x, [1, 2])\n",
    "            tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())\n",
    "            return x\n",
    "\n",
    "    ########## End ResNet class #############\n",
    "\n",
    "    ####### Subclassing ResNet specific to CIFAR-10 ###########\n",
    "\n",
    "    class ResNetCifar10(ResNet):\n",
    "        \"\"\"Cifar10 model with ResNetV1 and basic residual block.\"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                     num_layers,\n",
    "                     is_training,\n",
    "                     batch_norm_decay,\n",
    "                     batch_norm_epsilon,\n",
    "                     data_format='channels_first'):\n",
    "            super(ResNetCifar10, self).__init__(\n",
    "                is_training,\n",
    "                data_format,\n",
    "                batch_norm_decay,\n",
    "                batch_norm_epsilon\n",
    "            )\n",
    "            self.n = (num_layers - 2) // 6\n",
    "            # Add one in case label starts with 1. No impact if label starts with 0.\n",
    "            self.num_classes = 10\n",
    "            self.filters = [16, 16, 32, 64]\n",
    "            self.strides = [1, 2, 2]\n",
    "\n",
    "        def forward_pass(self, x, input_data_format='channels_last'):\n",
    "            \"\"\"Build the core model within the graph.\"\"\"\n",
    "            if self._data_format != input_data_format:\n",
    "                if input_data_format == 'channels_last':\n",
    "                    # Computation requires channels_first.\n",
    "                    x = tf.transpose(x, [0, 3, 1, 2])\n",
    "                else:\n",
    "                    # Computation requires channels_last.\n",
    "                    x = tf.transpose(x, [0, 2, 3, 1])\n",
    "\n",
    "            # Image standardization.\n",
    "            x = x / 128 - 1\n",
    "\n",
    "            x = self._conv(x, 3, 16, 1)\n",
    "            x = self._batch_norm(x)\n",
    "            x = self._relu(x)\n",
    "\n",
    "            # Use basic (non-bottleneck) block and ResNet V1 (post-activation).\n",
    "            res_func = self._residual_v1\n",
    "\n",
    "            # 3 stages of block stacking.\n",
    "            for i in range(3):\n",
    "                with tf.name_scope('stage'):\n",
    "                    for j in range(self.n):\n",
    "                        if j == 0:\n",
    "                            # First block in a stage, filters and strides may change.\n",
    "                            x = res_func(x, 3, self.filters[i], self.filters[i + 1],\n",
    "                                         self.strides[i])\n",
    "                        else:\n",
    "                            # Following blocks in a stage, constant filters and unit stride.\n",
    "                            x = res_func(x, 3, self.filters[i + 1], self.filters[i + 1], 1)\n",
    "\n",
    "            x = self._global_avg_pool(x)\n",
    "            x = self._fully_connected(x, self.num_classes)\n",
    "\n",
    "            return x\n",
    "    ####### End ResNetCifar10 class ###########\n",
    "\n",
    "    ######### Here we define all the hyperparameters, network, loss, optimzier and training operations ##################\n",
    "\n",
    "    ## Hyperparams\n",
    "    num_layers = 44\n",
    "\n",
    "    # batch_norm_decay = 0.997\n",
    "    batch_norm_decay = params[\"batch_norm_decay\"]\n",
    "    batch_norm_epsilon = 1e-5\n",
    "    # weight_decay = 2e-4\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    ## Neural network architecture\n",
    "    model = ResNetCifar10(\n",
    "        num_layers,\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=\"channels_last\")\n",
    "\n",
    "    data = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(\"data\", shape=(32,32,3))])\n",
    "    data = tf.reshape(data, (-1,32,32,3))\n",
    "    logits = model.forward_pass(data, input_data_format='channels_last')\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'logits': logits\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    ## Defining Loss\n",
    "#     labels = tf.string_to_number(labels,out_type=tf.int32)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    model_params = tf.trainable_variables()\n",
    "    loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "\n",
    "    ## Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, axis=1),\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec( mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    ## Create optimizer\n",
    "    num_batches_per_epoch = 45000 // 64\n",
    "    boundaries = [ num_batches_per_epoch * x for x in np.array([82, 123, 300], dtype=np.int64)]\n",
    "    staged_lr = [learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "    learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(), boundaries, staged_lr)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    ## Create global step and training operation\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    ## Some print operations for better logging.\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.control_dependencies([train_op]):\n",
    "            train_op = tf.Print(predictions, [predictions, tf.shape(predictions), \"predictions\"], summarize=32)\n",
    "            train_op = tf.Print(global_step, [global_step])\n",
    "\n",
    "    ## Return Estimator Spec with loss and training operation\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_chief_hooks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT :\n",
    "When trasfer learning just for changing the predict output following things are required:\n",
    "1. the predict ouput has to be a dictionary\n",
    "2. When training the estimator for previous checkpoint, keep the iterations low e.g.10, and learning_rate=0.0 \n",
    "so the the weights of the previous model are unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator2 = Estimator.NewClassificationEstimator(model_fn=cifar_model_function_only_logits_output)\n",
    "name2 = uniquify(\"CIFAR10_Logits_Only\")\n",
    "estimator2 = Estimator.create(name2, \"Resnet Cifar10 estimator\", estimator2)\n",
    "\n",
    "run_config2 = RunConfig(no_of_ps=1, no_of_workers=4, summary_save_frequency=10, run_eval=False, use_gpu=False)\n",
    "hyper_parameters2 = HParams(iterations=10,\n",
    "                       batch_size=8,\n",
    "                       batch_norm_decay=0.9,\n",
    "                       batch_norm_epsilon=1e-5,\n",
    "                       weight_decay=2e-4,\n",
    "                       learning_rate=0.0) \n",
    "experiment2_name = uniquify(\"CIFAR10_Logits_Only_Trial\")\n",
    "experiment2 = Experiment.run(experiment_name=experiment2_name,\n",
    "                     description=\"Cifar10 Model\",\n",
    "                     estimator=estimator2,\n",
    "                     hyper_parameters=hyper_parameters2,\n",
    "                     run_config=run_config2,\n",
    "                     dataset_version_split=None,\n",
    "                     input_function=input_function,\n",
    "                     transfer_learning_config=transfer_learning_config)\n",
    "job2 = experiment2.get_single_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2.list_jobs()\n",
    "experiment2.wait_until_finish()\n",
    "experiment2.report_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job2 = LearningJob.lookup(experiment2_name)\n",
    "model2 = job2.get_model()\n",
    "print(model2.model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching Model Inference Endpoint and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbrain.serving.model_service_endpoint import *\n",
    "\n",
    "end_point_name2=\"ep_{}\".format(model2.model_name)\n",
    "mep2 = ModelEndpoint.create(model=model2, \n",
    "                           endpoint_name=end_point_name2,\n",
    "                           description=end_point_name2, \n",
    "                           number_of_service_replicas=1, \n",
    "                           gpu_required=False)\n",
    "\n",
    "mep2.search(endpoint_name=end_point_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import base64\n",
    "def encode_img(img_path):\n",
    "    with open(img_path, \"rb\") as imageFile:\n",
    "        str = base64.b64encode(imageFile.read())\n",
    "        return str.decode(\"utf-8\")\n",
    "    \n",
    "with open(\"prediction_input.csv\", \"r\") as f:\n",
    "    records = f.readlines()\n",
    "\n",
    "records = list(map(lambda x: x.strip().split(\",\"), records))\n",
    " \n",
    "encoded_imgs = [encode_img(records[0][0])]\n",
    "\n",
    "features_dict = {}\n",
    "features_dict['features'] = encoded_imgs\n",
    "result = mep2.predict(features_dict)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
